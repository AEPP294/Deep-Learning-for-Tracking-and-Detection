<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable,
.markdown-body .highlighttable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr,
.markdown-body .highlighttable {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite,
.markdown-body .highlighttable pre,
.markdown-body .highlighttable div.highlight {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td,
.markdown-body .highlighttable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite,
.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*GitHub*/
.highlight {background-color:#fff;color:#333333;}
.highlight .hll {background-color:#ffffcc;}
.highlight .c{color:#999988;font-style:italic}
.highlight .err{color:#a61717;background-color:#e3d2d2}
.highlight .k{font-weight:bold}
.highlight .o{font-weight:bold}
.highlight .cm{color:#999988;font-style:italic}
.highlight .cp{color:#999999;font-weight:bold}
.highlight .c1{color:#999988;font-style:italic}
.highlight .cs{color:#999999;font-weight:bold;font-style:italic}
.highlight .gd{color:#000000;background-color:#ffdddd}
.highlight .ge{font-style:italic}
.highlight .gr{color:#aa0000}
.highlight .gh{color:#999999}
.highlight .gi{color:#000000;background-color:#ddffdd}
.highlight .go{color:#888888}
.highlight .gp{color:#555555}
.highlight .gs{font-weight:bold}
.highlight .gu{color:#800080;font-weight:bold}
.highlight .gt{color:#aa0000}
.highlight .kc{font-weight:bold}
.highlight .kd{font-weight:bold}
.highlight .kn{font-weight:bold}
.highlight .kp{font-weight:bold}
.highlight .kr{font-weight:bold}
.highlight .kt{color:#445588;font-weight:bold}
.highlight .m{color:#009999}
.highlight .s{color:#dd1144}
.highlight .n{color:#333333}
.highlight .na{color:teal}
.highlight .nb{color:#0086b3}
.highlight .nc{color:#445588;font-weight:bold}
.highlight .no{color:teal}
.highlight .ni{color:purple}
.highlight .ne{color:#990000;font-weight:bold}
.highlight .nf{color:#990000;font-weight:bold}
.highlight .nn{color:#555555}
.highlight .nt{color:navy}
.highlight .nv{color:teal}
.highlight .ow{font-weight:bold}
.highlight .w{color:#bbbbbb}
.highlight .mf{color:#009999}
.highlight .mh{color:#009999}
.highlight .mi{color:#009999}
.highlight .mo{color:#009999}
.highlight .sb{color:#dd1144}
.highlight .sc{color:#dd1144}
.highlight .sd{color:#dd1144}
.highlight .s2{color:#dd1144}
.highlight .se{color:#dd1144}
.highlight .sh{color:#dd1144}
.highlight .si{color:#dd1144}
.highlight .sx{color:#dd1144}
.highlight .sr{color:#009926}
.highlight .s1{color:#dd1144}
.highlight .ss{color:#990073}
.highlight .bp{color:#999999}
.highlight .vc{color:teal}
.highlight .vg{color:teal}
.highlight .vi{color:teal}
.highlight .il{color:#009999}
.highlight .gc{color:#999;background-color:#EAF2F5}
</style><title>ReadMe</title></head><body><article class="markdown-body"><!-- No Heading Fix -->

<p>Collection of papers, datasets, code and other resources for object detection and tracking using deep learning
<!-- MarkdownTOC --></p>
<ul>
<li><a href="#research_data_">Research Data</a></li>
<li><a href="#paper_s_">Papers</a><ul>
<li><a href="#static_detectio_n_">Static Detection</a><ul>
<li><a href="#region_proposal_">Region Proposal</a></li>
<li><a href="#rcn_n_">RCNN</a></li>
<li><a href="#yol_o_">YOLO</a></li>
<li><a href="#ssd_">SSD</a></li>
<li><a href="#retinanet_">RetinaNet</a></li>
<li><a href="#anchor_free_">Anchor Free</a></li>
<li><a href="#mis_c_">Misc</a></li>
</ul>
</li>
<li><a href="#video_detectio_n_">Video Detection</a><ul>
<li><a href="#tubelet_">Tubelet</a></li>
<li><a href="#fgf_a_">FGFA</a></li>
<li><a href="#rnn_">RNN</a></li>
</ul>
</li>
<li><a href="#multi_object_tracking_">Multi Object Tracking</a><ul>
<li><a href="#joint_detection_">Joint-Detection</a><ul>
<li><a href="#identity_embeddin_g_">Identity Embedding</a></li>
</ul>
</li>
<li><a href="#association_">Association</a></li>
<li><a href="#deep_learning_">Deep Learning</a></li>
<li><a href="#rnn__1">RNN</a></li>
<li><a href="#unsupervised_learning_">Unsupervised Learning</a></li>
<li><a href="#reinforcement_learning_">Reinforcement Learning</a></li>
<li><a href="#network_flow_">Network Flow</a></li>
<li><a href="#graph_optimization_">Graph Optimization</a></li>
<li><a href="#baselin_e_">Baseline</a></li>
<li><a href="#metrics_">Metrics</a></li>
</ul>
</li>
<li><a href="#single_object_tracking_">Single Object Tracking</a><ul>
<li><a href="#reinforcement_learning__1">Reinforcement Learning</a></li>
<li><a href="#siamese_">Siamese</a></li>
<li><a href="#correlation_">Correlation</a></li>
<li><a href="#mis_c__1">Misc</a></li>
</ul>
</li>
<li><a href="#deep_learning__1">Deep Learning</a><ul>
<li><a href="#synthetic_gradient_s_">Synthetic Gradients</a></li>
<li><a href="#efficient_">Efficient</a></li>
</ul>
</li>
<li><a href="#unsupervised_learning__1">Unsupervised Learning</a></li>
<li><a href="#interpolation_">Interpolation</a></li>
<li><a href="#autoencoder_">Autoencoder</a><ul>
<li><a href="#variational_">Variational</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#dataset_s_">Datasets</a><ul>
<li><a href="#multi_object_tracking__1">Multi Object Tracking</a><ul>
<li><a href="#uav_">UAV</a></li>
<li><a href="#synthetic_">Synthetic</a></li>
<li><a href="#microscopy___cell_tracking_">Microscopy / Cell Tracking</a></li>
</ul>
</li>
<li><a href="#single_object_tracking__1">Single Object Tracking</a></li>
<li><a href="#video_detectio_n__1">Video Detection</a><ul>
<li><a href="#video_understanding___activity_recognitio_n_">Video Understanding / Activity Recognition</a></li>
</ul>
</li>
<li><a href="#static_detectio_n__1">Static Detection</a><ul>
<li><a href="#animals_">Animals</a></li>
</ul>
</li>
<li><a href="#boundary_detectio_n_">Boundary Detection</a></li>
<li><a href="#static_segmentation_">Static Segmentation</a></li>
<li><a href="#video_segmentation_">Video Segmentation</a></li>
<li><a href="#classificatio_n_">Classification</a></li>
<li><a href="#optical_flow_">Optical Flow</a></li>
<li><a href="#motion_prediction_">Motion Prediction</a></li>
</ul>
</li>
<li><a href="#cod_e_">Code</a><ul>
<li><a href="#general_vision_">General Vision</a></li>
<li><a href="#multi_object_tracking__2">Multi Object Tracking</a><ul>
<li><a href="#framework_s_">Frameworks</a></li>
<li><a href="#general_">General</a></li>
<li><a href="#baselin_e__1">Baseline</a></li>
<li><a href="#siamese__1">Siamese</a></li>
<li><a href="#unsupervise_d_">Unsupervised</a></li>
<li><a href="#re_id_">Re-ID</a><ul>
<li><a href="#framework_s__1">Frameworks</a></li>
</ul>
</li>
<li><a href="#graph_nn_">Graph NN</a></li>
<li><a href="#microscopy___cell_tracking__1">Microscopy / cell tracking</a></li>
<li><a href="#3_d_">3D</a></li>
<li><a href="#metrics__1">Metrics</a></li>
</ul>
</li>
<li><a href="#single_object_tracking__2">Single Object Tracking</a><ul>
<li><a href="#gui_application___large_scale_tracking___animal_s_">GUI Application / Large Scale Tracking / Animals</a></li>
</ul>
</li>
<li><a href="#video_detectio_n__2">Video Detection</a><ul>
<li><a href="#action_detectio_n_">Action Detection</a><ul>
<li><a href="#framework_s__2">Frameworks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#static_detection_and_matching_">Static Detection and Matching</a><ul>
<li><a href="#framework_s__3">Frameworks</a></li>
<li><a href="#region_proposal__1">Region Proposal</a></li>
<li><a href="#fpn_">FPN</a></li>
<li><a href="#rcn_n__1">RCNN</a></li>
<li><a href="#ssd__1">SSD</a></li>
<li><a href="#retinanet__1">RetinaNet</a></li>
<li><a href="#yol_o__1">YOLO</a></li>
<li><a href="#anchor_free__1">Anchor Free</a></li>
<li><a href="#mis_c__2">Misc</a></li>
<li><a href="#matchin_g_">Matching</a></li>
<li><a href="#boundary_detectio_n__1">Boundary Detection</a></li>
<li><a href="#text_detectio_n_">Text Detection</a><ul>
<li><a href="#framework_s__4">Frameworks</a></li>
</ul>
</li>
<li><a href="#3d_detectio_n_">3D Detection</a><ul>
<li><a href="#framework_s__5">Frameworks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#optical_flow__1">Optical Flow</a><ul>
<li><a href="#framework_s__6">Frameworks</a></li>
</ul>
</li>
<li><a href="#instance_segmentation_">Instance Segmentation</a><ul>
<li><a href="#framework_s__7">Frameworks</a></li>
</ul>
</li>
<li><a href="#semantic_segmentation_">Semantic Segmentation</a><ul>
<li><a href="#framework_s__8">Frameworks</a></li>
<li><a href="#polyp_">Polyp</a></li>
</ul>
</li>
<li><a href="#panoptic_segmentation_">Panoptic Segmentation</a></li>
<li><a href="#video_segmentation__1">Video Segmentation</a><ul>
<li><a href="#panoptic_video_segmentation_">Panoptic Video Segmentation</a></li>
</ul>
</li>
<li><a href="#motion_prediction__1">Motion Prediction</a></li>
<li><a href="#pose_estimation_">Pose Estimation</a><ul>
<li><a href="#framework_s__9">Frameworks</a></li>
</ul>
</li>
<li><a href="#autoencoder_s_">Autoencoders</a></li>
<li><a href="#classificatio_n__1">Classification</a><ul>
<li><a href="#framework_s__10">Frameworks</a></li>
</ul>
</li>
<li><a href="#deep_rl_">Deep RL</a></li>
<li><a href="#annotatio_n_">Annotation</a><ul>
<li><a href="#editing_">Editing</a></li>
<li><a href="#augmentatio_n_">Augmentation</a></li>
</ul>
</li>
<li><a href="#deep_learning__2">Deep Learning</a><ul>
<li><a href="#class_imbalanc_e_">Class Imbalance</a></li>
<li><a href="#few_shot_learning_">Few shot learning</a></li>
<li><a href="#unsupervised_learning__2">Unsupervised learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#collections_">Collections</a><ul>
<li><a href="#dataset_s__1">Datasets</a></li>
<li><a href="#deep_learning__3">Deep Learning</a></li>
<li><a href="#static_detectio_n__2">Static Detection</a></li>
<li><a href="#video_detectio_n__3">Video Detection</a></li>
<li><a href="#single_object_tracking__3">Single Object Tracking</a></li>
<li><a href="#multi_object_tracking__3">Multi Object Tracking</a></li>
<li><a href="#static_segmentation__1">Static Segmentation</a></li>
<li><a href="#video_segmentation__2">Video Segmentation</a></li>
<li><a href="#motion_prediction__2">Motion Prediction</a></li>
<li><a href="#deep_compressed_sensin_g_">Deep Compressed Sensing</a></li>
<li><a href="#mis_c__3">Misc</a></li>
</ul>
</li>
<li><a href="#tutorials_">Tutorials</a><ul>
<li><a href="#collections__1">Collections</a></li>
<li><a href="#multi_object_tracking__4">Multi Object Tracking</a></li>
<li><a href="#static_detectio_n__3">Static Detection</a></li>
<li><a href="#video_detectio_n__4">Video Detection</a></li>
<li><a href="#instance_segmentation__1">Instance Segmentation</a></li>
<li><a href="#deep_learning__4">Deep Learning</a><ul>
<li><a href="#optimizatio_n_">Optimization</a></li>
<li><a href="#class_imbalanc_e__1">Class Imbalance</a></li>
</ul>
</li>
<li><a href="#rnn__2">RNN</a></li>
<li><a href="#deep_rl__1">Deep RL</a></li>
<li><a href="#autoencoder_s__1">Autoencoders</a></li>
</ul>
</li>
<li><a href="#blogs_">Blogs</a></li>
</ul>
<!-- /MarkdownTOC -->

<p><a id="research_data_"></a></p>
<h1 id="research-data">Research Data<a class="headerlink" href="#research-data" title="Permanent link"></a></h1>
<p>I use <a href="http://www.davidrm.com/">DavidRM Journal</a> for managing my research data for its excellent hierarchical organization, cross-linking and tagging capabilities.</p>
<p>I make available a Journal entry export file that contains tagged and categorized collection of papers, articles, tutorials, code and notes about computer vision and deep learning that I have collected over the last few years.</p>
<p>This is what the topic cloud looks like:
<img alt="Alt text" src="/Z:/UofA/PhD/Literature/topic_cloud.jpg?raw=true" /></p>
<p>It needs Jounal 8 and can be imported using following steps:</p>
<ul>
<li>Import my user preferences using <strong>File</strong> -&gt; <strong>Import</strong> -&gt; <strong>Import User Preferences</strong></li>
<li>Import research data using <strong>File</strong> -&gt; <strong>Import</strong> -&gt; <strong>Sync from The Journal Export File</strong></li>
</ul>
<p>Note that my user preferences must be imported <em>before</em> the research data for the tagged topics to work correctly.</p>
<p>(optional) My global options file is also provided for those interested in a dark theme and can be imported using <strong>File</strong> -&gt; <strong>Import</strong> -&gt; <strong>Import Global Options</strong></p>
<ul>
<li><a href="/Z:/UofA/PhD/Literature/research_data/user_settings.juser">User Preferences</a></li>
<li><a href="/Z:/UofA/PhD/Literature/research_data/phd_literature_readings.tjexp">Entry Export File</a></li>
<li><a href="/Z:/UofA/PhD/Literature/research_data/global_options.tjglobal">Global Options</a></li>
</ul>
<p>Updated: 2023-11-22</p>
<p><a id="paper_s_"></a></p>
<h1 id="papers">Papers<a class="headerlink" href="#papers" title="Permanent link"></a></h1>
<p><a id="static_detectio_n_"></a></p>
<h2 id="static-detection">Static Detection<a class="headerlink" href="#static-detection" title="Permanent link"></a></h2>
<p><a id="region_proposal_"></a></p>
<h3 id="region-proposal">Region Proposal<a class="headerlink" href="#region-proposal" title="Permanent link"></a></h3>
<ul>
<li><strong>Scalable Object Detection Using Deep Neural Networks</strong>
[cvpr14]
<a href="/Z:/UofA/PhD/Literature/static_detection/region_proposal/Scalable%20Object%20Detection%20Using%20Deep%20Neural%20Networks%20cvpr14.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Scalable%20Object%20Detection%20Using%20Deep%20Neural%20Networks%20cvpr14.pdf">[notes]</a></li>
<li><strong>Selective Search for Object Recognition</strong>
[ijcv2013]
<a href="/Z:/UofA/PhD/Literature/static_detection/region_proposal/Selective%20Search%20for%20Object%20Recognition%20ijcv2013.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Selective%20Search%20for%20Object%20Recognition%20ijcv2013.pdf">[notes]</a></li>
</ul>
<p><a id="rcn_n_"></a></p>
<h3 id="rcnn">RCNN<a class="headerlink" href="#rcnn" title="Permanent link"></a></h3>
<ul>
<li><strong>Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks</strong>
[tpami17]
<a href="/Z:/UofA/PhD/Literature/static_detection/RCNN/Faster%20R-CNN%20Towards%20Real-Time%20Object%20Detection%20with%20Region%20Proposal%20Networks%20tpami17%20ax16_1.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Faster_R-CNN.pdf">[notes]</a></li>
<li><strong>RFCN - Object Detection via Region-based Fully Convolutional Networks</strong>
[nips16]
[Microsoft Research]
<a href="/Z:/UofA/PhD/Literature/static_detection/RCNN/RFCN-Object%20Detection%20via%20Region-based%20Fully%20Convolutional%20Networks%20nips16.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/RFCN.pdf">[notes]</a>  </li>
<li><strong>Mask R-CNN</strong>
[iccv17]
[Facebook AI Research]
<a href="/Z:/UofA/PhD/Literature/static_detection/RCNN/Mask%20R-CNN%20ax17_4%20iccv17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Mask%20R-CNN%20ax17_4%20iccv17.pdf">[notes]</a>
<a href="https://arxiv.org/abs/1703.06870">[arxiv]</a>
<a href="https://github.com/matterport/Mask_RCNN">[code (keras)]</a>
<a href="https://github.com/CharlesShang/FastMaskRCNN">[code (tensorflow)]</a></li>
<li><strong>SNIPER Efficient Multi-Scale Training</strong>
[ax1812/nips18]
<a href="/Z:/UofA/PhD/Literature/static_detection/RCNN/SNIPER%20Efficient%20Multi-Scale%20Training%20ax181213%20nips18.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/SNIPER%20Efficient%20Multi-Scale%20Training%20ax181213%20nips18.pdf">[notes]</a>
<a href="https://github.com/mahyarnajibi/SNIPER">[code]</a></li>
</ul>
<p><a id="yol_o_"></a></p>
<h3 id="yolo">YOLO<a class="headerlink" href="#yolo" title="Permanent link"></a></h3>
<ul>
<li><strong>You Only Look Once Unified, Real-Time Object Detection</strong>
[ax1605]
<a href="/Z:/UofA/PhD/Literature/static_detection/yolo/You%20Only%20Look%20Once%20Unified%2C%20Real-Time%20Object%20Detection%20ax1605.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/You%20Only%20Look%20Once%20Unified%2C%20Real-Time%20Object%20Detection%20ax1605.pdf">[notes]</a></li>
<li><strong>YOLO9000 Better, Faster, Stronger</strong>
[ax1612]
<a href="/Z:/UofA/PhD/Literature/static_detection/yolo/YOLO9000%20Better%2C%20Faster%2C%20Stronger%20ax16_12.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/YOLO9000%20Better%2C%20Faster%2C%20Stronger%20ax16_12.pdf">[notes]</a></li>
<li><strong>YOLOv3 An Incremental Improvement</strong>
[ax1804]
<a href="/Z:/UofA/PhD/Literature/static_detection/yolo/YOLOv3%20An%20Incremental%20Improvement%20ax180408.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/YOLOv3%20An%20Incremental%20Improvement%20ax180408.pdf">[notes]</a></li>
<li><strong>YOLOv4 Optimal Speed and Accuracy of Object Detection</strong>
[ax2004]
<a href="/Z:/UofA/PhD/Literature/static_detection/yolo/YOLOV4_Optimal%20Speed%20and%20Accuracy%20of%20Object%20Detection%20ax200423.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/YOLOV4_Optimal%20Speed%20and%20Accuracy%20of%20Object%20Detection%20ax200423.pdf">[notes]</a>
<a href="https://github.com/AlexeyAB/darknet">[code]</a></li>
</ul>
<p><a id="ssd_"></a></p>
<h3 id="ssd">SSD<a class="headerlink" href="#ssd" title="Permanent link"></a></h3>
<ul>
<li><strong>SSD Single Shot MultiBox Detector</strong>
[ax1612/eccv16]
<a href="/Z:/UofA/PhD/Literature/static_detection/ssd/SSD%20Single%20Shot%20MultiBox%20Detector%20eccv16_ax16_12.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/SSD.pdf">[notes]</a></li>
<li><strong>DSSD  Deconvolutional Single Shot Detector</strong>
[ax1701]
<a href="/Z:/UofA/PhD/Literature/static_detection/ssd/DSSD%20Deconvolutional%20Single%20Shot%20Detector%20ax1701.06659.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/DSSD.pdf">[notes]</a></li>
</ul>
<p><a id="retinanet_"></a></p>
<h3 id="retinanet">RetinaNet<a class="headerlink" href="#retinanet" title="Permanent link"></a></h3>
<ul>
<li><strong>Feature Pyramid Networks for Object Detection</strong>
[ax1704]
<a href="/Z:/UofA/PhD/Literature/static_detection/retinanet/Feature%20Pyramid%20Networks%20for%20Object%20Detection%20ax170419.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/FPN.pdf">[notes]</a></li>
<li><strong>Focal Loss for Dense Object Detection</strong>
[ax180207/iccv17]
<a href="/Z:/UofA/PhD/Literature/static_detection/retinanet/Focal%20Loss%20for%20Dense%20Object%20Detection%20ax180207%20iccv17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/focal_loss.pdf">[notes]</a> </li>
</ul>
<p><a id="anchor_free_"></a></p>
<h3 id="anchor-free">Anchor Free<a class="headerlink" href="#anchor-free" title="Permanent link"></a></h3>
<ul>
<li><strong>FoveaBox: Beyond Anchor-based Object Detector</strong>
[ax1904]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/FoveaBox%20Beyond%20Anchor-based%20Object%20Detector%20ax1904.03797.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/FoveaBox%20Beyond%20Anchor-based%20Object%20Detector%20ax1904.03797.pdf">[notes]</a>
<a href="https://github.com/taokong/FoveaBox">[code]</a></li>
<li><strong>CornerNet: Detecting Objects as Paired Keypoints</strong>
[ax1903/ijcv19]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/CornerNet%20Detecting%20Objects%20as%20Paired%20Keypoints%20ax1903%20ijcv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/CornerNet%20Detecting%20Objects%20as%20Paired%20Keypoints%20ax1903%20ijcv19.pdf">[notes]</a>
<a href="https://github.com/princeton-vl/CornerNet">[code]</a></li>
<li><strong>FCOS Fully Convolutional One-Stage Object Detection</strong>
[ax1908/iccv19]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/FCOS%20Fully%20Convolutional%20One-Stage%20Object%20Detection%20ax1908%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/FCOS%20Fully%20Convolutional%20One-Stage%20Object%20Detection%20ax1908%20iccv19.pdf">[notes]</a>
<a href="https://github.com/tianzhi0549/FCOS">[code]</a>
<a href="https://github.com/yqyao/FCOS_PLUS">[code/FCOS_PLUS]</a>
<a href="https://github.com/vov-net/VoVNet-FCOS">[code/VoVNet]</a>
<a href="https://github.com/HRNet/HRNet-FCOS">[code/HRNet]</a>
<a href="https://github.com/Lausannen/NAS-FCOS">[code/NAS]</a></li>
<li><strong>Feature Selective Anchor-Free Module for Single-Shot Object Detection</strong>
[ax1903/cvpr19]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/Feature%20Selective%20Anchor-Free%20Module%20for%20Single-Shot%20Object%20Detection%20ax1903.00621%20cvpr19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Feature%20Selective%20Anchor-Free%20Module%20for%20Single-Shot%20Object%20Detection%20ax1903.00621%20cvpr19.pdf">[notes]</a>
<a href="https://github.com/hdjang/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection">[code]</a></li>
<li><strong>Bottom-up object detection by grouping extreme and center points</strong>
[ax1901]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/Bottom-up%20object%20detection%20by%20grouping%20extreme%20and%20center%20points%201901.08043.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Bottom-up%20object%20detection%20by%20grouping%20extreme%20and%20center%20points%201901.08043.pdf">[notes]</a>
<a href="https://github.com/xingyizhou/ExtremeNet">[code]</a></li>
<li><strong>Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</strong>
[ax1912/cvpr20]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/Bridging%20the%20Gap%20Between%20Anchor-based%20and%20Anchor-free%20Detection%20via%20Adaptive%20Training%20Sample%20Selection%201912.02424%20cvpr20.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Bridging%20the%20Gap%20Between%20Anchor-based%20and%20Anchor-free%20Detection%20via%20Adaptive%20Training%20Sample%20Selection%201912.02424%20cvpr20.pdf">[notes]</a>
<a href="https://github.com/sfzhang15/ATSS">[code]</a></li>
<li><strong>End-to-end object detection with Transformers</strong>
[ax200528]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/End-to-End%20Object%20Detection%20with%20Transformers%20ax200528.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/End-to-end%20object%20detection%20with%20Transformers%20ax200528.pdf">[notes]</a>
<a href="https://github.com/facebookresearch/detr">[code]</a></li>
<li><strong>Objects as Points</strong>
[ax1904]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/Objects%20as%20Points%20ax1904.07850.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Objects%20as%20Points%20ax1904.07850.pdf">[notes]</a>
<a href="https://github.com/xingyizhou/CenterNet">[code]</a></li>
<li><strong>RepPoints Point Set Representation for Object Detection</strong>
[iccv19]
<a href="/Z:/UofA/PhD/Literature/static_detection/anchor_free/RepPoints%20Point%20Set%20Representation%20for%20Object%20Detection%201904.11490%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/RepPoints%20Point%20Set%20Representation%20for%20Object%20Detection%201904.11490%20iccv19.pdf">[notes]</a>
<a href="https://github.com/microsoft/RepPoints">[code]</a></li>
</ul>
<p><a id="mis_c_"></a></p>
<h3 id="misc">Misc<a class="headerlink" href="#misc" title="Permanent link"></a></h3>
<ul>
<li><strong>OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks</strong>
[ax1402/iclr14]
<a href="/Z:/UofA/PhD/Literature/static_detection/OverFeat%20Integrated%20Recognition%2C%20Localization%20and%20Detection%20using%20Convolutional%20Networks%20ax1402%20iclr14.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/OverFeat%20Integrated%20Recognition%2C%20Localization%20and%20Detection%20using%20Convolutional%20Networks%20ax1402%20iclr14.pdf">[notes]</a></li>
<li><strong>LSDA Large scale detection through adaptation</strong>
[ax1411/nips14]
<a href="/Z:/UofA/PhD/Literature/static_detection/LSDA%20Large%20scale%20detection%20through%20adaptation%20nips14%20ax14_11.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/LSDA%20Large%20scale%20detection%20through%20adaptation%20nips14%20ax14_11.pdf">[notes]</a></li>
<li><strong>Acquisition of Localization Confidence for Accurate Object Detection</strong>
[ax1807/eccv18]
<a href="/Z:/UofA/PhD/Literature/static_detection/Acquisition%20of%20Localization%20Confidence%20for%20Accurate%20Object%20Detection%201807.11590%20eccv18.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/IOU-Net.pdf">[notes]</a>
<a href="https://github.com/vacancy/PreciseRoIPooling">[code]</a></li>
<li><strong>EfficientDet: Scalable and Efficient Object Detection</strong>
[cvpr20]
<a href="/Z:/UofA/PhD/Literature/static_detection/EfficientDet_Scalable%20and%20efficient%20object%20detection.pdf">[pdf]</a></li>
<li><strong>Generalized Intersection over Union A Metric and A Loss for Bounding Box Regression</strong>
[ax1902/cvpr19]
<a href="/Z:/UofA/PhD/Literature/static_detection/Generalized%20Intersection%20over%20Union%20A%20Metric%20and%20A%20Loss%20for%20Bounding%20Box%20Regression%201902.09630%20cvpr19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/static_detection/notes/Generalized%20Intersection%20over%20Union%20A%20Metric%20and%20A%20Loss%20for%20Bounding%20Box%20Regression%201902.09630%20cvpr19.pdf">[notes]</a>
<a href="https://github.com/generalized-iou">[code]</a>
<a href="https://giou.stanford.edu/">[project]</a></li>
</ul>
<p><a id="video_detectio_n_"></a></p>
<h2 id="video-detection">Video Detection<a class="headerlink" href="#video-detection" title="Permanent link"></a></h2>
<p><a id="tubelet_"></a></p>
<h3 id="tubelet">Tubelet<a class="headerlink" href="#tubelet" title="Permanent link"></a></h3>
<ul>
<li><strong>Object Detection from Video Tubelets with Convolutional Neural Networks</strong>
[cvpr16]
<a href="/Z:/UofA/PhD/Literature/video_detection/tubelets/Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_CVPR16.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_CVPR16.pdf">[notes]</a></li>
<li><strong>Object Detection in Videos with Tubelet Proposal Networks</strong>
[ax1704/cvpr17]
<a href="/Z:/UofA/PhD/Literature/video_detection/tubelets/Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_ax1704_cvpr17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_ax1704_cvpr17.pdf">[notes]</a></li>
</ul>
<p><a id="fgf_a_"></a></p>
<h3 id="fgfa">FGFA<a class="headerlink" href="#fgfa" title="Permanent link"></a></h3>
<ul>
<li><strong>Deep Feature Flow for Video Recognition</strong>
[cvpr17]
[Microsoft Research]
<a href="/Z:/UofA/PhD/Literature/video_detection/fgfa/Deep%20Feature%20Flow%20For%20Video%20Recognition%20cvpr17.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1611.07715">[arxiv]</a>
<a href="https://github.com/msracver/Deep-Feature-Flow">[code]</a>   </li>
<li><strong>Flow-Guided Feature Aggregation for Video Object Detection</strong>
[ax1708/iccv17]
<a href="/Z:/UofA/PhD/Literature/video_detection/fgfa/Flow-Guided%20Feature%20Aggregation%20for%20Video%20Object%20Detection%20ax1708%20iccv17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Flow-Guided%20Feature%20Aggregation%20for%20Video%20Object%20Detection%20ax1708%20iccv17.pdf">[notes]</a></li>
<li><strong>Towards High Performance Video Object Detection</strong>
[ax1711]
[Microsoft]
<a href="/Z:/UofA/PhD/Literature/video_detection/fgfa/Towards%20High%20Performance%20Video%20Object%20Detection%20ax171130%20microsoft.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Towards%20High%20Performance%20Video%20Object%20Detection%20ax171130%20microsoft.pdf">[notes]</a></li>
</ul>
<p><a id="rnn_"></a></p>
<h3 id="rnn">RNN<a class="headerlink" href="#rnn" title="Permanent link"></a></h3>
<ul>
<li><strong>Online Video Object Detection using Association LSTM</strong>
[iccv17]
<a href="/Z:/UofA/PhD/Literature/video_detection/rnn/Online%20Video%20Object%20Detection%20using%20Association%20LSTM%20iccv17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Online%20Video%20Object%20Detection%20using%20Association%20LSTM%20iccv17.pdf">[notes]</a></li>
<li><strong>Context Matters Reﬁning Object Detection in Video with Recurrent Neural Networks</strong>
[bmvc16]
<a href="/Z:/UofA/PhD/Literature/video_detection/rnn/Context%20Matters%20Re%EF%AC%81ning%20Object%20Detection%20in%20Video%20with%20Recurrent%20Neural%20Networks%20bmvc16.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/video_detection/notes/Context%20Matters%20Re%EF%AC%81ning%20Object%20Detection%20in%20Video%20with%20Recurrent%20Neural%20Networks%20bmvc16.pdf">[notes]</a></li>
</ul>
<p><a id="multi_object_tracking_"></a></p>
<h2 id="multi-object-tracking">Multi Object Tracking<a class="headerlink" href="#multi-object-tracking" title="Permanent link"></a></h2>
<p><a id="joint_detection_"></a></p>
<h3 id="joint-detection">Joint-Detection<a class="headerlink" href="#joint-detection" title="Permanent link"></a></h3>
<ul>
<li><strong>Tracking Objects as Points</strong>
[ax2004]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/joint_detection/Tracking%20Objects%20as%20Points%202004.01177.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Tracking%20Objects%20as%20Points%202004.01177.pdf">[notes]</a>
<a href="https://github.com/xingyizhou/CenterTrack">[code]</a>[pytorch]</li>
</ul>
<p><a id="identity_embeddin_g_"></a></p>
<h4 id="identity-embedding">Identity Embedding<a class="headerlink" href="#identity-embedding" title="Permanent link"></a></h4>
<ul>
<li><strong>MOTS Multi-Object Tracking and Segmentation</strong>
[cvpr19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/joint_detection/MOTS%20Multi-Object%20Tracking%20and%20Segmentation%20ax1904%20cvpr19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/MOTS%20Multi-Object%20Tracking%20and%20Segmentation%20ax1904%20cvpr19.pdf">[notes]</a>
<a href="https://github.com/VisualComputingInstitute/TrackR-CNN">[code]</a>
<a href="https://www.vision.rwth-aachen.de/page/mots">[project/data]</a></li>
<li><strong>Towards Real-Time Multi-Object Tracking</strong>
[ax1909]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/joint_detection/Towards%20Real-Time%20Multi-Object%20Tracking%20ax1909.12605v1.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Towards%20Real-Time%20Multi-Object%20Tracking%20ax1909.12605v1.pdf">[notes]</a></li>
<li>
<p><strong>A Simple Baseline for Multi-Object Tracking</strong>
[ax2004]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/joint_detection/A%20Simple%20Baseline%20for%20Multi-Object%20Tracking%202004.01888.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/A%20Simple%20Baseline%20for%20Multi-Object%20Tracking%202004.01888.pdf">[notes]</a>
<a href="https://github.com/ifzhang/FairMOT">[code]</a></p>
</li>
<li>
<p><strong>Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</strong>
[ax1811]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/joint_detection/Integrated%20Object%20Detection%20and%20Tracking%20with%20Tracklet-Conditioned%20Detection%201811.11167.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Integrated%20Object%20Detection%20and%20Tracking%20with%20Tracklet-Conditioned%20Detection%201811.11167.pdf">[notes]</a></p>
</li>
</ul>
<p><a id="association_"></a></p>
<h3 id="association">Association<a class="headerlink" href="#association" title="Permanent link"></a></h3>
<ul>
<li><strong>Deep Affinity Network for Multiple Object Tracking</strong>
[ax1810/tpami19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/association/Deep%20Affinity%20Network%20for%20Multiple%20Object%20Tracking%20ax1810.11780%20tpami19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Deep%20Affinity%20Network%20for%20Multiple%20Object%20Tracking%20ax1810.11780%20tpami19.pdf">[notes]</a>
<a href="https://github.com/shijieS/SST">[code]</a> [pytorch]</li>
</ul>
<p><a id="deep_learning_"></a></p>
<h3 id="deep-learning">Deep Learning<a class="headerlink" href="#deep-learning" title="Permanent link"></a></h3>
<ul>
<li><strong>Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism</strong>
[ax1708/iccv17]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/deep_learning/Online%20Multi-Object%20Tracking%20Using%20CNN-based%20Single%20Object%20Tracker%20with%20Spatial-Temporal%20Attention%20Mechanism%201708.02843%20iccv17.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1708.02843">[arxiv]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Online%20Multi-Object%20Tracking%20Using%20CNN-based%20Single%20Object%20Tracker%20with%20Spatial-Temporal%20Attention%20Mechanism%201708.02843%20iccv17.pdf">[notes]</a></li>
<li><strong>Online multi-object tracking with dual matching attention networks</strong>
[ax1902/eccv18]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/deep_learning/Online%20multi-object%20tracking%20with%20dual%20matching%20attention%20networks%201902.00749%20eccv18.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1902.00749">[arxiv]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Online%20multi-object%20tracking%20with%20dual%20matching%20attention%20networks%201902.00749%20eccv18.pdf">[notes]</a>
<a href="https://github.com/jizhu1023/DMAN_MOT">[code]</a></li>
<li>
<p><strong>FAMNet Joint Learning of Feature, Affinity and Multi-Dimensional Assignment for Online Multiple Object Tracking</strong>
[iccv19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/deep_learning/FAMNet%20Joint%20Learning%20of%20Feature%2C%20Affinity%20and%20Multi-Dimensional%20Assignment%20for%20Online%20Multiple%20Object%20Tracking%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/FAMNet%20Joint%20Learning%20of%20Feature%2C%20Affinity%20and%20Multi-Dimensional%20Assignment%20for%20Online%20Multiple%20Object%20Tracking%20iccv19.pdf">[notes]</a></p>
</li>
<li>
<p><strong>Exploit the Connectivity: Multi-Object Tracking with TrackletNet</strong>
[ax1811/mm19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/deep_learning/Exploit%20the%20Connectivity%20Multi-Object%20Tracking%20with%20TrackletNet%20ax1811.07258%20mm19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Exploit%20the%20Connectivity%20Multi-Object%20Tracking%20with%20TrackletNet%20ax1811.07258%20mm19.pdf">[notes]</a></p>
</li>
<li><strong>Tracking without bells and whistles</strong>
[ax1903/iccv19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/deep_learning/Tracking%20without%20bells%20and%20whistles%20ax1903.05625%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Tracking%20without%20bells%20and%20whistles%20ax1903.05625%20iccv19.pdf">[notes]</a>
<a href="https://github.com/phil-bergmann/tracking_wo_bnw">[code]</a> [pytorch]</li>
</ul>
<p><a id="rnn__1"></a></p>
<h3 id="rnn_1">RNN<a class="headerlink" href="#rnn_1" title="Permanent link"></a></h3>
<ul>
<li><strong>Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies</strong>
[ax1704/iccv17]
[Stanford]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/rnn/Tracking%20The%20Untrackable%20Learning%20To%20Track%20Multiple%20Cues%20with%20Long-Term%20Dependencies%20ax17_4_iccv17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Tracking_The_Untrackable_Learning_To_Track_Multiple_Cues_with_Long-Term_Dependencies.pdf">[notes]</a>
<a href="https://arxiv.org/abs/1701.01909">[arxiv]</a>
<a href="http://web.stanford.edu/~alahi/">[project]</a>,</li>
<li><strong>Multi-object Tracking with Neural Gating Using Bilinear LSTM</strong>
[eccv18]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/rnn/Multi-object%20Tracking%20with%20Neural%20Gating%20Using%20Bilinear%20LSTM_eccv18.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Multi-object%20Tracking%20with%20Neural%20Gating%20Using%20Bilinear%20LSTM_eccv18.pdf">[notes]</a></li>
<li><strong>Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</strong>
[cvpr19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/rnn/Eliminating%20Exposure%20Bias%20and%20Metric%20Mismatch%20in%20Multiple%20Object%20Tracking%20cvpr19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Eliminating%20Exposure%20Bias%20and%20Metric%20Mismatch%20in%20Multiple%20Object%20Tracking%20cvpr19.pdf">[notes]</a>
<a href="https://github.com/maksay/seq-train">[code]</a></li>
</ul>
<p><a id="unsupervised_learning_"></a></p>
<h3 id="unsupervised-learning">Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Permanent link"></a></h3>
<ul>
<li><strong>Unsupervised Person Re-identification by Deep Learning Tracklet Association</strong>
[ax1809/eccv18]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/unsupervised/Unsupervised%20Person%20Re-identification%20by%20Deep%20Learning%20Tracklet%20Association%201809.02874%20eccv18.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Unsupervised%20Person%20Re-identification%20by%20Deep%20Learning%20Tracklet%20Association%201809.02874%20eccv18.pdf">[notes]</a></li>
<li><strong>Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers</strong>
[ax1809/cvpr19]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/unsupervised/Tracking%20by%20Animation%20Unsupervised%20Learning%20of%20Multi-Object%20Attentive%20Trackers%20cvpr19%20ax1809.03137.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1809.03137">[arxiv]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Tracking%20by%20Animation%20Unsupervised%20Learning%20of%20Multi-Object%20Attentive%20Trackers%20cvpr19%20ax1809.03137.pdf">[notes]</a>
<a href="https://github.com/zhen-he/tracking-by-animation">[code]</a></li>
<li><strong>Simple Unsupervised Multi-Object Tracking</strong>
[ax2006]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/unsupervised/Simple%20Unsupervised%20Multi-Object%20Tracking%202006.02609.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Simple%20Unsupervised%20Multi-Object%20Tracking%202006.02609.pdf">[notes]</a></li>
</ul>
<p><a id="reinforcement_learning_"></a></p>
<h3 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link"></a></h3>
<ul>
<li><strong>Learning to Track: Online Multi-object Tracking by Decision Making</strong>
[iccv15]
[Stanford]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/rl/Learning%20to%20Track%20Online%20Multi-object%20Tracking%20by%20Decision%20Making%20%20iccv15.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Learning_to_Track_Online_Multi-object_Tracking_by_Decision_Making__iccv15.pdf">[notes]</a>
<a href="https://github.com/yuxng/MDP_Tracking">[code (matlab)]</a>
<a href="https://yuxng.github.io/">[project]</a></li>
<li><strong>Collaborative Deep Reinforcement Learning for Multi-Object Tracking</strong>
[eccv18]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/rl/Collaborative%20Deep%20Reinforcement%20Learning%20for%20Multi-Object%20Tracking_eccv18.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Collaborative%20Deep%20Reinforcement%20Learning%20for%20Multi-Object%20Tracking_eccv18.pdf">[notes]</a></li>
</ul>
<p><a id="network_flow_"></a></p>
<h3 id="network-flow">Network Flow<a class="headerlink" href="#network-flow" title="Permanent link"></a></h3>
<ul>
<li><strong>Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</strong>
[iccv15]
[NEC Labs]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/network_flow/Near-online%20multi-target%20tracking%20with%20aggregated%20local%20%EF%AC%82ow%20descriptor%20iccv15.pdf">[pdf]</a>
<a href="http://www-personal.umich.edu/~wgchoi/">[author]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/NOMT.pdf">[notes]</a>  </li>
<li><strong>Deep Network Flow for Multi-Object Tracking</strong>
[cvpr17]
[NEC Labs]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/network_flow/Deep%20Network%20Flow%20for%20Multi-Object%20Tracking%20cvpr17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/network_flow/Deep%20Network%20Flow%20for%20Multi-Object%20Tracking%20cvpr17_supplemental.pdf">[supplementary]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Deep%20Network%20Flow%20for%20Multi-Object%20Tracking%20cvpr17.pdf">[notes]</a>  </li>
<li><strong>Learning a Neural Solver for Multiple Object Tracking</strong>
[ax1912/cvpr20]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/network_flow/Learning%20a%20Neural%20Solver%20for%20Multiple%20Object%20Tracking%201912.07515%20cvpr20.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Learning%20a%20Neural%20Solver%20for%20Multiple%20Object%20Tracking%201912.07515%20cvpr20.pdf">[notes]</a>
<a href="https://github.com/dvl-tum/mot_neural_solver">[code]</a></li>
</ul>
<p><a id="graph_optimization_"></a></p>
<h3 id="graph-optimization">Graph Optimization<a class="headerlink" href="#graph-optimization" title="Permanent link"></a></h3>
<ul>
<li><strong>A Multi-cut Formulation for Joint Segmentation and Tracking of Multiple Objects</strong>
[ax1607]
[highest MT on MOT2015]
[University of Freiburg, Germany]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/batch/A%20Multi-cut%20Formulation%20for%20Joint%20Segmentation%20and%20Tracking%20of%20Multiple%20Objects%20ax16_9%20%5Bbest%20MT%20on%20MOT15%5D.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1607.06317">[arxiv]</a>
<a href="https://lmb.informatik.uni-freiburg.de/people/keuper/publications.html">[author]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/A_Multi-cut_Formulation_for_Joint_Segmentation_and_Tracking_of_Multiple_Objects.pdf">[notes]</a></li>
</ul>
<p><a id="baselin_e_"></a></p>
<h3 id="baseline">Baseline<a class="headerlink" href="#baseline" title="Permanent link"></a></h3>
<ul>
<li><strong>Simple Online and Realtime Tracking</strong>
[icip16]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/baseline/Simple%20Online%20and%20Realtime%20Tracking%20ax1707%20icip16.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/Simple%20Online%20and%20Realtime%20Tracking%20ax1707%20icip16.pdf">[notes]</a>
<a href="https://github.com/abewley/sort">[code]</a></li>
<li><strong>High-Speed Tracking-by-Detection Without Using Image Information</strong>
[avss17]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/baseline/High-Speed%20Tracking-by-Detection%20Without%20Using%20Image%20Information%20avss17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/High-Speed%20Tracking-by-Detection%20Without%20Using%20Image%20Information%20avss17.pdf">[notes]</a>
<a href="https://github.com/bochinski/iou-tracker">[code]</a></li>
</ul>
<p><a id="metrics_"></a></p>
<h3 id="metrics">Metrics<a class="headerlink" href="#metrics" title="Permanent link"></a></h3>
<ul>
<li><strong>HOTA A Higher Order Metric for Evaluating Multi-object Tracking</strong>
[ijcv20/08]
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/metrics/HOTA%20A%20Higher%20Order%20Metric%20for%20Evaluating%20Multi-object%20Tracking%20sl_open_2010_ijcv2008.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/multi_object_tracking/notes/HOTA%20A%20Higher%20Order%20Metric%20for%20Evaluating%20Multi-object%20Tracking%20sl_open_2010_ijcv2008.pdf">[notes]</a>
<a href="https://github.com/JonathonLuiten/HOTA-metrics">[code]</a></li>
</ul>
<p><a id="single_object_tracking_"></a></p>
<h2 id="single-object-tracking">Single Object Tracking<a class="headerlink" href="#single-object-tracking" title="Permanent link"></a></h2>
<p><a id="reinforcement_learning__1"></a></p>
<h3 id="reinforcement-learning_1">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning_1" title="Permanent link"></a></h3>
<ul>
<li><strong>Deep Reinforcement Learning for Visual Object Tracking in Videos</strong>
[ax1704] [USC-Santa Barbara, Samsung Research]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/reinforcement_learning/Deep%20Reinforcement%20Learning%20for%20Visual%20Object%20Tracking%20in%20Videos%20ax17_4.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1701.08936">[arxiv]</a>
<a href="http://www.cs.ucsb.edu/~dazhang/">[author]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/Deep_Reinforcement_Learning_for_Visual_Object_Tracking_in_Videos.pdf">[notes]</a>  </li>
<li><strong>Visual Tracking by Reinforced Decision Making</strong>
[ax1702] [Seoul National University, Chung-Ang University]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/reinforcement_learning/Visual%20Tracking%20by%20Reinforced%20Decision%20Making%20ax17_2.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1702.06291">[arxiv]</a>
<a href="http://cau.ac.kr/~jskwon/">[author]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/Visual_Tracking_by_Reinforced_Decision_Making_ax17.pdf">[notes]</a></li>
<li><strong>Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning</strong>
[cvpr17] [Seoul National University]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/reinforcement_learning/Action-Decision%20Networks%20for%20Visual%20Tracking%20with%20Deep%20Reinforcement%20Learning%20%20cvpr17%20supplementary.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/reinforcement_learning/Action-Decision%20Networks%20for%20Visual%20Tracking%20with%20Deep%20Reinforcement%20Learning%20%20cvpr17.pdf">[supplementary]</a>
<a href="https://sites.google.com/view/cvpr2017-adnet">[project]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/Action-Decision_Networks_for_Visual_Tracking_with_Deep_Reinforcement_Learning_cvpr17.pdf">[notes]</a> 
<a href="https://github.com/ildoonet/tf-adnet-tracking">[code]</a> </li>
<li><strong>End-to-end Active Object Tracking via Reinforcement Learning</strong>
[ax1705]
[Peking University, Tencent AI Lab]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/reinforcement_learning/End-to-end%20Active%20Object%20Tracking%20via%20Reinforcement%20Learning%20ax17_5.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/1705.10561">[arxiv]</a></li>
</ul>
<p><a id="siamese_"></a></p>
<h3 id="siamese">Siamese<a class="headerlink" href="#siamese" title="Permanent link"></a></h3>
<ul>
<li><strong>Fully-Convolutional Siamese Networks for Object Tracking</strong>
[eccv16]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/siamese/Fully-Convolutional%20Siamese%20Networks%20for%20Object%20Tracking%20eccv16_9.pdf">[pdf]</a>
<a href="https://www.robots.ox.ac.uk/~luca/siamese-fc.html">[project]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/SiameseFC.pdf">[notes]</a>  </li>
<li><strong>High Performance Visual Tracking with Siamese Region Proposal Network</strong>
[cvpr18]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/siamese/High%20Performance%20Visual%20Tracking%20with%20Siamese%20Region%20Proposal%20Network_cvpr18.pdf">[pdf]</a>
<a href="http://www.robots.ox.ac.uk/~qwang/">[author]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/High%20Performance%20Visual%20Tracking%20with%20Siamese%20Region%20Proposal%20Network_cvpr18.pdf">[notes]</a>  </li>
<li><strong>Siam R-CNN Visual Tracking by Re-Detection</strong>
[cvpr20]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/siamese/Siam%20R-CNN%20Visual%20Tracking%20by%20Re-Detection%201911.12836%20cvpr20.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/Siam%20R-CNN%20Visual%20Tracking%20by%20Re-Detection%201911.12836%20cvpr20.pdf">[notes]</a>
<a href="https://www.vision.rwth-aachen.de/page/siamrcnn">[project]</a>
<a href="https://github.com/VisualComputingInstitute/SiamR-CNN">[code]</a>  </li>
</ul>
<p><a id="correlation_"></a></p>
<h3 id="correlation">Correlation<a class="headerlink" href="#correlation" title="Permanent link"></a></h3>
<ul>
<li><strong>ATOM Accurate Tracking by Overlap Maximization</strong>
[cvpr19]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/correlation/ATOM%20Accurate%20Tracking%20by%20Overlap%20Maximization%20ax1811.07628%20cvpr19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/ATOM%20Accurate%20Tracking%20by%20Overlap%20Maximization%20ax1811.07628%20cvpr19.pdf">[notes]</a>
<a href="https://github.com/visionml/pytracking">[code]</a></li>
<li><strong>DiMP Learning Discriminative Model Prediction for Tracking</strong>
[iccv19]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/correlation/DiMP%20Learning%20Discriminative%20Model%20Prediction%20for%20Tracking%20ax1904.07220%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/DiMP%20Learning%20Discriminative%20Model%20Prediction%20for%20Tracking%20ax1904.07220%20iccv19.pdf">[notes]</a>
<a href="https://github.com/visionml/pytracking">[code]</a></li>
<li><strong>D3S – A Discriminative Single Shot Segmentation Tracker</strong>
[cvpr20]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/correlation/D3S%20%E2%80%93%20A%20Discriminative%20Single%20Shot%20Segmentation%20Tracker%201911.08862v1%20cvpr20.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/D3S%20%E2%80%93%20A%20Discriminative%20Single%20Shot%20Segmentation%20Tracker%201911.08862v1%20cvpr20.pdf">[notes]</a>
<a href="https://github.com/alanlukezic/d3s">[code]</a></li>
</ul>
<p><a id="mis_c__1"></a></p>
<h3 id="misc_1">Misc<a class="headerlink" href="#misc_1" title="Permanent link"></a></h3>
<ul>
<li><strong>Bridging the Gap Between Detection and Tracking A Unified Approach</strong>
[iccv19]
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/Bridging%20the%20Gap%20Between%20Detection%20and%20Tracking%20A%20Unified%20Approach%20iccv19.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/single_object_tracking/notes/Bridging%20the%20Gap%20Between%20Detection%20and%20Tracking%20A%20Unified%20Approach%20iccv19.pdf">[notes]</a></li>
</ul>
<p><a id="deep_learning__1"></a></p>
<h2 id="deep-learning_1">Deep Learning<a class="headerlink" href="#deep-learning_1" title="Permanent link"></a></h2>
<ul>
<li><strong>Do Deep Nets Really Need to be Deep</strong>
[nips14]
<a href="/Z:/UofA/PhD/Literature/deep_learning/theory/Do%20Deep%20Nets%20Really%20Need%20to%20be%20Deep%20ax1410%20nips14.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/deep_learning/notes/Do%20Deep%20Nets%20Really%20Need%20to%20be%20Deep%20ax1410%20nips14.pdf">[notes]</a></li>
</ul>
<p><a id="synthetic_gradient_s_"></a></p>
<h3 id="synthetic-gradients">Synthetic Gradients<a class="headerlink" href="#synthetic-gradients" title="Permanent link"></a></h3>
<ul>
<li><strong>Decoupled Neural Interfaces using Synthetic Gradients</strong>
[ax1608]
<a href="/Z:/UofA/PhD/Literature/deep_learning/synthetic_gradients/Decoupled%20Neural%20Interfaces%20using%20Synthetic%20Gradients%20ax1608.05343.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/deep_learning/notes/Decoupled%20Neural%20Interfaces%20using%20Synthetic%20Gradients%20ax1608.05343.pdf">[notes]</a>    </li>
<li><strong>Understanding Synthetic Gradients and Decoupled Neural Interfaces</strong>
[ax1703]
<a href="/Z:/UofA/PhD/Literature/deep_learning/synthetic_gradients/Understanding%20Synthetic%20Gradients%20and%20Decoupled%20Neural%20Interfaces%20ax1703.00522.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/deep_learning/notes/Understanding%20Synthetic%20Gradients%20and%20Decoupled%20Neural%20Interfaces%20ax1703.00522.pdf">[notes]</a></li>
</ul>
<p><a id="efficient_"></a></p>
<h3 id="efficient">Efficient<a class="headerlink" href="#efficient" title="Permanent link"></a></h3>
<ul>
<li><strong>EfficientNet:Rethinking Model Scaling for Convolutional Neural Networks</strong>
[icml2019]
<a href="/Z:/UofA/PhD/Literature/deep_learning/efficient/EfficientNet_Rethinking%20model%20scaling%20for%20CNNs.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/deep_learning/notes/EfficientNet_%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks.pdf">[notes]</a></li>
</ul>
<p><a id="unsupervised_learning__1"></a></p>
<h2 id="unsupervised-learning_1">Unsupervised Learning<a class="headerlink" href="#unsupervised-learning_1" title="Permanent link"></a></h2>
<ul>
<li><strong>Learning Features by Watching Objects Move</strong>
(cvpr17)
<a href="/Z:/UofA/PhD/Literature/unsupervised/segmentation/Learning%20Features%20by%20Watching%20Objects%20Move%20ax170412%20cvpr17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/unsupervised/notes/Learning%20Features%20by%20Watching%20Objects%20Move%20ax170412%20cvpr17.pdf">[notes]</a></li>
</ul>
<p><a id="interpolation_"></a></p>
<h2 id="interpolation">Interpolation<a class="headerlink" href="#interpolation" title="Permanent link"></a></h2>
<ul>
<li><strong>Video Frame Interpolation via Adaptive Convolution</strong>
[cvpr17 / iccv17]
<a href="/Z:/UofA/PhD/Literature/interpolation/Video%20Frame%20Interpolation%20via%20Adaptive%20Convolution%20ax1703.pdf">[pdf (cvpr17)]</a>
<a href="/Z:/UofA/PhD/Literature/interpolation/Video%20Frame%20Interpolation%20via%20Adaptive%20Separable%20Convolution%20iccv17.pdf">[pdf (iccv17)]</a>
<a href="/Z:/UofA/PhD/Literature/interpolation/notes/Video%20Frame%20Interpolation%20via%20Adaptive%20Convolution%20ax1703.pdf">[ppt]</a></li>
</ul>
<p><a id="autoencoder_"></a></p>
<h2 id="autoencoder">Autoencoder<a class="headerlink" href="#autoencoder" title="Permanent link"></a></h2>
<p><a id="variational_"></a></p>
<h3 id="variational">Variational<a class="headerlink" href="#variational" title="Permanent link"></a></h3>
<ul>
<li><strong>beta-VAE Learning Basic Visual Concepts with a Constrained Variational Framework</strong> [iclr17]
<a href="/Z:/UofA/PhD/Literature/autoencoder/variational/beta-VAE%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%20iclr17.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/autoencoder/notes/beta-VAE%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%20iclr17.pdf">[notes]</a></li>
<li><strong>Disentangling by Factorising</strong> [ax1806]
<a href="/Z:/UofA/PhD/Literature/autoencoder/variational/Disentangling%20by%20Factorising%20ax1806.pdf">[pdf]</a>
<a href="/Z:/UofA/PhD/Literature/autoencoder/notes/Disentangling%20by%20Factorising%20ax1806.pdf">[notes]</a>  </li>
</ul>
<p><a id="dataset_s_"></a></p>
<h1 id="datasets">Datasets<a class="headerlink" href="#datasets" title="Permanent link"></a></h1>
<p><a id="multi_object_tracking__1"></a></p>
<h2 id="multi-object-tracking_1">Multi Object Tracking<a class="headerlink" href="#multi-object-tracking_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://www.cs.uic.edu/Bits/YanziJin">IDOT</a></li>
<li><a href="http://detrac-db.rit.albany.edu/">UA-DETRAC Benchmark Suite</a></li>
<li><a href="http://agamenon.tsc.uah.es/Personales/rlopez/data/rtm/">GRAM Road-Traffic Monitoring</a></li>
<li><a href="http://www.uni-ulm.de/in/mrm/forschung/datensaetze.html">Ko-PER Intersection Dataset</a></li>
<li><a href="http://agamenon.tsc.uah.es/Personales/rlopez/data/trancos/">TRANCOS</a></li>
<li><a href="https://www.jpjodoin.com/urbantracker/dataset.html">Urban Tracker</a></li>
<li><a href="http://vision.cse.psu.edu/data/vividEval/datasets/datasets.html">DARPA VIVID / PETS 2005</a> [Non stationary camera]</li>
<li><a href="http://i21www.ira.uka.de/image_sequences/">KIT-AKS</a> [No ground truth]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/streetscenes/">CBCL StreetScenes Challenge Framework</a> [No top down viewpoint]</li>
<li><a href="https://motchallenge.net/data/2D_MOT_2015/">MOT 2015</a> [mostly street level viewpoint]</li>
<li><a href="https://motchallenge.net/data/MOT16/">MOT 2016</a> [mostly street level viewpoint]</li>
<li><a href="https://motchallenge.net/data/MOT17/">MOT 2017</a> [mostly street level viewpoint]</li>
<li><a href="https://motchallenge.net/data/MOT20/">MOT 2020</a> [mostly top down  viewpoint]</li>
<li><a href="https://www.vision.rwth-aachen.de/page/mots">MOTS: Multi-Object Tracking and Segmentation</a> [MOT and KITTI]</li>
<li><a href="https://motchallenge.net/data/11">CVPR 2019</a> [mostly street level viewpoint]</li>
<li><a href="http://www.cvg.reading.ac.uk/PETS2009/a.html">PETS 2009</a> [No vehicles]</li>
<li><a href="https://motchallenge.net/data/PETS2017/">PETS 2017</a> [Low density] [mostly pedestrians]</li>
<li><a href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC</a> [multi camera] [static background] [pedestrians] [above-street level viewpoint] [website not working]</li>
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php">KITTI Tracking Dataset</a> [No top down viewpoint] [non stationary camera]</li>
<li><a href="https://cvlab.epfl.ch/data/data-wildtrack/">The WILDTRACK Seven-Camera HD Dataset</a> [pedestrian detection and tracking]</li>
<li><a href="http://www.cvlibs.net/projects/intersection/">3D Traffic Scene Understanding from Movable Platforms</a> [intersection traffic] [stereo setup] [moving camera]</li>
<li><a href="http://lost.cse.wustl.edu/">LOST : Longterm Observation of Scenes with Tracks</a> [top down and street level viewpoint] [no ground truth]</li>
<li><a href="http://imagelab.ing.unimore.it/imagelab/page.asp?IdPage=25">JTA</a> [top down and street level viewpoint] [synthetic/GTA 5] [pedestrian] [3D annotations]</li>
<li><a href="http://people.ee.ethz.ch/~daid/pathtrack/">PathTrack: Fast Trajectory Annotation with Path Supervision</a> [top down and street level viewpoint] [iccv17] [pedestrian] </li>
<li><a href="https://www.aicitychallenge.org/">CityFlow</a> [pole mounted] [intersections] [vehicles] [re-id]  [cvpr19]</li>
<li><a href="https://jrdb.stanford.edu/">JackRabbot Dataset</a>  [RGBD] [head-on][indoor/outdoor][stanford]</li>
<li><a href="http://taodataset.org/">TAO: A Large-Scale Benchmark for Tracking Any Object</a>  [eccv20] <a href="https://github.com/TAO-Dataset/tao">[code]</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/OFFICEDATA/">Edinburgh office monitoring video dataset</a>  [indoors][long term][mostly static people]</li>
<li><a href="https://waymo.com/open/">Waymo Open Dataset</a>  [outdoors][vehicles]</li>
</ul>
<p><a id="uav_"></a></p>
<h3 id="uav">UAV<a class="headerlink" href="#uav" title="Permanent link"></a></h3>
<ul>
<li><a href="http://cvgl.stanford.edu/projects/uav_data/">Stanford Drone Dataset</a></li>
<li><a href="https://sites.google.com/site/daviddo0323/projects/uavdt">UAVDT - The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking</a> [uav] [intersections/highways] [vehicles]  [eccv18]</li>
<li><a href="https://github.com/VisDrone/VisDrone-Dataset">VisDrone</a> </li>
</ul>
<p><a id="synthetic_"></a></p>
<h3 id="synthetic">Synthetic<a class="headerlink" href="#synthetic" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/zhen-he/tracking-by-animation">MNIST-MOT / MNIST-Sprites </a>  [script generated] [cvpr19]</li>
<li><a href="https://www.nue.tu-berlin.de/menue/forschung/software_und_datensaetze/mocat/">TUB Multi-Object and Multi-Camera Tracking Dataset </a>  [avss16]</li>
<li><a href="http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds">Virtual KITTI</a> <a href="https://arxiv.org/abs/1605.06457">[arxiv]</a>   [cvpr16] [link seems broken]</li>
</ul>
<p><a id="microscopy___cell_tracking_"></a></p>
<h3 id="microscopy-cell-tracking">Microscopy / Cell Tracking<a class="headerlink" href="#microscopy-cell-tracking" title="Permanent link"></a></h3>
<ul>
<li><a href="http://celltrackingchallenge.net/">Cell Tracking Challenge</a>  [nature methods/2017]</li>
<li><a href="https://ivc.ischool.utexas.edu/ctmc/">CTMC: Cell Tracking with Mitosis Detection Dataset Challenge </a>  [cvprw20] <a href="https://motchallenge.net/data/CTMC-v1/">[MOT]</a></li>
</ul>
<p><a id="single_object_tracking__1"></a></p>
<h2 id="single-object-tracking_1">Single Object Tracking<a class="headerlink" href="#single-object-tracking_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://tracking-net.org/">TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild</a> [eccv18]</li>
<li><a href="https://cis.temple.edu/lasot/">LaSOT: Large-scale Single Object Tracking</a> [cvpr19]</li>
<li><a href="http://ci2cv.net/nfs/index.html">Need for speed: A benchmark for higher frame rate object tracking</a> [iccv17]</li>
<li><a href="https://oxuva.github.io/long-term-tracking-benchmark/">Long-term Tracking in the Wild A Benchmark</a> [eccv18]</li>
<li><a href="https://uav123.org/">UAV123: A benchmark and simulator for UAV tracking</a> [eccv16] <a href="https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx">[project]</a></li>
<li><a href="https://sim4cv.org/">Sim4CV A Photo-Realistic Simulator for Computer Vision Applications</a> [ijcv18]   </li>
<li><a href="https://www.vicos.si/Projects/CDTB">CDTB: A Color and Depth Visual Object Tracking and Benchmark</a> [iccv19]   [RGBD]</li>
<li><a href="http://www.dabi.temple.edu/~hbling/data/TColor-128/TColor-128.html">Temple Color 128 - Color Tracking Benchmark</a> [tip15]</li>
</ul>
<p><a id="video_detectio_n__1"></a></p>
<h2 id="video-detection_1">Video Detection<a class="headerlink" href="#video-detection_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://research.google.com/youtube-bb/download.html">YouTube-BB</a></li>
<li><a href="http://bvisionweb1.cs.unc.edu/ilsvrc2015/download-videos-3j16.php">Imagenet-VID</a></li>
</ul>
<p><a id="video_understanding___activity_recognitio_n_"></a></p>
<h3 id="video-understanding-activity-recognition">Video Understanding / Activity Recognition<a class="headerlink" href="#video-understanding-activity-recognition" title="Permanent link"></a></h3>
<ul>
<li><a href="https://research.google.com/youtube8m/">YouTube-8M</a></li>
<li><a href="https://research.google.com/ava/">AVA: A Video Dataset of Atomic Visual Action</a></li>
<li><a href="http://www.viratdata.org/">VIRAT Video Dataset</a></li>
<li><a href="https://deepmind.com/research/open-source/kinetics">Kinetics Action Recognition Dataset</a></li>
</ul>
<p><a id="static_detectio_n__1"></a></p>
<h2 id="static-detection_1">Static Detection<a class="headerlink" href="#static-detection_1" title="Permanent link"></a></h2>
<ul>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL Visual Object Classes</a></li>
<li><a href="https://github.com/PKU-IMRE/VERI-Wild">A Large-Scale Dataset for Vehicle Re-Identification in the Wild</a>
[cvpr19]</li>
<li><a href="https://github.com/ahrnbom/ViratAnnotationObjectDetection">Object Detection-based annotations for some frames of the VIRAT dataset</a></li>
<li><a href="http://podoce.dinf.usherbrooke.ca/challenge/dataset/">MIO-TCD: A new benchmark dataset for vehicle classification and localization</a> [tip18]</li>
<li><a href="https://tiny-imagenet.herokuapp.com/">Tiny ImageNet</a></li>
</ul>
<p><a id="animals_"></a></p>
<h3 id="animals">Animals<a class="headerlink" href="#animals" title="Permanent link"></a></h3>
<ul>
<li><a href="https://lev.cs.rpi.edu/public/datasets/wild.tar.gz">Wildlife Image and Localization Dataset (species and bounding box labels)</a>
[wacv18]</li>
<li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a>
[cvpr11]</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford-IIIT Pet Dataset</a>
[cvpr12]</li>
<li><a href="http://www.vision.caltech.edu/visipedia/CUB-200.html">Caltech-UCSD Birds 200</a> [rough segmentation] [attributes]</li>
<li><a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP/TPB5ID">Gold Standard Snapshot Serengeti Bounding Box Coordinates</a></li>
</ul>
<p><a id="boundary_detectio_n_"></a></p>
<h2 id="boundary-detection">Boundary Detection<a class="headerlink" href="#boundary-detection" title="Permanent link"></a></h2>
<ul>
<li><a href="http://home.bharathh.info/pubs/codes/SBD/download.html">Semantic Boundaries Dataset and Benchmark</a></li>
</ul>
<p><a id="static_segmentation_"></a></p>
<h2 id="static-segmentation">Static Segmentation<a class="headerlink" href="#static-segmentation" title="Permanent link"></a></h2>
<ul>
<li><a href="http://cocodataset.org/#download">COCO - Common Objects in Context</a></li>
<li><a href="https://storage.googleapis.com/openimages/web/index.html">Open Images</a></li>
<li><a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a> [cvpr17]</li>
<li><a href="http://synthia-dataset.net/download-2/">SYNTHIA</a> [cvpr16]</li>
<li><a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html">UC Berkeley Computer Vision Group - Contour Detection and Image Segmentation</a></li>
</ul>
<p><a id="video_segmentation_"></a></p>
<h2 id="video-segmentation">Video Segmentation<a class="headerlink" href="#video-segmentation" title="Permanent link"></a></h2>
<ul>
<li><a href="https://davischallenge.org/">DAVIS: Densely Annotated VIdeo Segmentation</a></li>
<li><a href="https://www.mapillary.com/dataset/vistas?pKey=0_xJqX3-c-KyTb90oG_8HQ&amp;lat=20&amp;lng=0&amp;z=1.5">Mapillary Vistas Dataset</a> [street scenes] [semi-free]</li>
<li><a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/">BDD100K</a> [street scenes] [autonomous driving]</li>
<li><a href="http://apolloscape.auto/">ApolloScape</a> [street scenes] [autonomous driving]</li>
<li><a href="https://www.cityscapes-dataset.com/">Cityscapes</a> [street scenes] [instance-level]</li>
<li><a href="https://youtube-vos.org/dataset/vis/">YouTube-VOS</a> [iccv19]</li>
</ul>
<p><a id="classificatio_n_"></a></p>
<h2 id="classification">Classification<a class="headerlink" href="#classification" title="Permanent link"></a></h2>
<ul>
<li><a href="http://www.image-net.org/challenges/LSVRC/2012/">ImageNet Large Scale Visual Recognition Competition 2012</a></li>
<li><a href="https://cvml.ist.ac.at/AwA2/">Animals with Attributes 2</a></li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html">CompCars Dataset</a></li>
<li><a href="https://objectnet.dev/">ObjectNet</a> [only test set]</li>
</ul>
<p><a id="optical_flow_"></a></p>
<h2 id="optical-flow">Optical Flow<a class="headerlink" href="#optical-flow" title="Permanent link"></a></h2>
<ul>
<li><a href="http://vision.middlebury.edu/flow/data/">Middlebury</a></li>
<li><a href="http://sintel.is.tue.mpg.de/">MPI Sintel</a></li>
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow">KITTI Flow</a></li>
</ul>
<p><a id="motion_prediction_"></a></p>
<h2 id="motion-prediction">Motion Prediction<a class="headerlink" href="#motion-prediction" title="Permanent link"></a></h2>
<ul>
<li><a href="https://www.aicrowd.com/challenges/trajnet-a-trajectory-forecasting-challenge">Trajnet++ (A Trajectory Forecasting Challenge)</a></li>
<li><a href="http://trajnet.stanford.edu/">Trajectory Forecasting Challenge</a></li>
</ul>
<p><a id="cod_e_"></a></p>
<h1 id="code">Code<a class="headerlink" href="#code" title="Permanent link"></a></h1>
<p><a id="general_vision_"></a></p>
<h2 id="general-vision">General Vision<a class="headerlink" href="#general-vision" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/dmlc/gluon-cv">Gluon CV Toolkit</a> [mxnet] [pytorch]</li>
<li><a href="https://github.com/open-mmlab/mmcv">OpenMMLab Computer Vision Foundation</a> [pytorch]</li>
</ul>
<p><a id="multi_object_tracking__2"></a></p>
<h2 id="multi-object-tracking_2">Multi Object Tracking<a class="headerlink" href="#multi-object-tracking_2" title="Permanent link"></a></h2>
<p><a id="framework_s_"></a></p>
<h3 id="frameworks">Frameworks<a class="headerlink" href="#frameworks" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmtracking">OpenMMLab Video Perception Toolbox. It supports Video Object Detection (VID), Multiple Object Tracking (MOT), Single Object Tracking (SOT), Video Instance Segmentation (VIS) with a unified framework</a> [pytorch]</li>
</ul>
<p><a id="general_"></a></p>
<h3 id="general">General<a class="headerlink" href="#general" title="Permanent link"></a></h3>
<ul>
<li><a href="http://www.csee.umbc.edu/~hpirsiav/papers/tracking_release_v1.0.tar.gz">Globally-optimal greedy algorithms for tracking a variable number of objects</a> [cvpr11] [matlab] <a href="https://www.csee.umbc.edu/~hpirsiav/">[author]</a>    </li>
<li><a href="https://bitbucket.org/amilan/contracking">Continuous Energy Minimization for Multitarget Tracking</a> [cvpr11 / iccv11 / tpami  2014] [matlab]</li>
<li><a href="http://www.milanton.de/files/software/dctracking-v1.0.zip">Discrete-Continuous Energy Minimization for Multi-Target Tracking</a> [cvpr12] [matlab] <a href="http://www.milanton.de/dctracking/index.html">[project]</a></li>
<li><a href="https://bitbucket.org/cdicle/smot/src/master/">The way they move: Tracking multiple targets with similar appearance</a> [iccv13] [matlab]   </li>
<li><a href="http://www.cvlibs.net/projects/intersection/">3D Traffic Scene Understanding from Movable Platforms</a> <a href="http://www.cvlibs.net/software/trackbydet/">[2d_tracking]</a> [pami14/kit13/iccv13/nips11] [c++/matlab]</li>
<li><a href="http://www.cbsr.ia.ac.cn/users/lywen/codes/MultiCarTracker.zip">Multiple target tracking based on undirected hierarchical relation hypergraph</a> [cvpr14] [C++] <a href="http://www.cbsr.ia.ac.cn/users/lywen/">[author]</a></li>
<li><a href="https://drive.google.com/open?id=1YMqvkrVI6LOXRwcaUlAZTu_b2_5GmTAM">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</a> [cvpr14] [matlab] <a href="https://sites.google.com/view/inuvision/research">(project)</a></li>
<li><a href="https://github.com/yuxng/MDP_Tracking">Learning to Track: Online Multi-Object Tracking by Decision Making</a> [iccv15] [matlab]</li>
<li><a href="https://bitbucket.org/amilan/segtracking">Joint Tracking and Segmentation of Multiple Targets</a> [cvpr15] [matlab]</li>
<li><a href="http://rehg.org/mht/">Multiple Hypothesis Tracking Revisited</a> [iccv15] [highest MT on MOT2015 among open source trackers] [matlab]</li>
<li><a href="https://github.com/aljosaosep/ciwt">Combined Image- and World-Space Tracking in Traffic Scenes</a> [icra 2017] [c++]</li>
<li><a href="https://bitbucket.org/amilan/rnntracking/src/default/">Online Multi-Target Tracking with Recurrent Neural Networks</a> [aaai17] [lua/torch7]</li>
<li><a href="https://github.com/samuelmurray/tracking-by-detection">Real-Time Multiple Object Tracking - A Study on the Importance of Speed</a> [ax1710/masters thesis] [c++]        </li>
<li><a href="https://github.com/JunaidCS032/MOTBeyondPixels">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</a> [icra18] [matlab]    </li>
<li><a href="https://github.com/jizhu1023/DMAN_MOT">Online Multi-Object Tracking with Dual Matching Attention Network</a> [eccv18] [matlab/tensorflow]    </li>
<li><a href="https://github.com/VisualComputingInstitute/TrackR-CNN">TrackR-CNN - Multi-Object Tracking and Segmentation</a> [cvpr19] [tensorflow] <a href="https://www.vision.rwth-aachen.de/page/mots">[project]</a> </li>
<li><a href="https://github.com/maksay/seq-train">Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</a> [cvpr19] [tensorflow]    </li>
<li><a href="https://github.com/ZwwWayne/mmMOT">Robust Multi-Modality Multi-Object Tracking</a> [iccv19] [pytorch]    </li>
<li><a href="https://github.com/Zhongdao/Towards-Realtime-MOT">Towards Real-Time Multi-Object Tracking / Joint Detection and Embedding</a> [ax1909] [pytorch] <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">[CMU]</a></li>
<li><a href="https://github.com/shijieS/SST">Deep Affinity Network for Multiple Object Tracking</a> [tpami19] [pytorch]    </li>
<li><a href="https://github.com/phil-bergmann/tracking_wo_bnw">Tracking without bells and whistles</a> [iccv19] [pytorch]    </li>
<li><a href="https://github.com/AndreaHor/LifT_Solver">Lifted Disjoint Paths with Application in Multiple Object Tracking</a> [icml20] [matlab] [mot15#1,mot16 #3,mot17#2]   </li>
<li><a href="https://github.com/dvl-tum/mot_neural_solver">Learning a Neural Solver for Multiple Object Tracking</a> [cvpr20] [pytorch] [mot15#2]   </li>
<li><a href="https://github.com/xingyizhou/CenterTrack">Tracking Objects as Points</a> [ax2004] [pytorch]</li>
<li><a href="https://github.com/SysCV/qdtrack">Quasi-Dense Similarity Learning for Multiple Object Tracking</a> [ax2006] [pytorch]</li>
<li><a href="https://github.com/MedChaabane/DEFT">DEFT: Detection Embeddings for Tracking</a> [ax2102] [pytorch]</li>
<li><a href="https://github.com/yihongXU/deepMOT">How To Train Your Deep Multi-Object Tracker</a> [ax1906/cvpr20] [pytorch] <a href="https://gitlab.inria.fr/yixu/deepmot">[traktor/gitlab]</a></li>
<li><a href="https://github.com/JialianW/TraDeS">Track To Detect and Segment: An Online Multi-Object Tracker </a> [cvpr21] [pytorch] <a href="https://jialianwu.com/projects/TraDeS.html">[project]</a></li>
<li><a href="https://github.com/megvii-model/MOTR">MOTR: End-to-End Multiple-Object Tracking with Transformer</a> [ax2202] [pytorch]</li>
</ul>
<p><a id="baselin_e__1"></a></p>
<h3 id="baseline_1">Baseline<a class="headerlink" href="#baseline_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/abewley/sort">Simple Online and Realtime Tracking</a> [icip 2016] [python]</li>
<li><a href="https://github.com/nwojke/deep_sort">Deep SORT : Simple Online Realtime Tracking with a Deep Association Metric</a> [icip17] [python]</li>
<li><a href="https://github.com/bochinski/iou-tracker">High-Speed Tracking-by-Detection Without Using Image Information</a> [avss17] [python]  </li>
<li><a href="https://github.com/ifzhang/FairMOT">A simple baseline for one-shot multi-object tracking</a> [ax2004] [pytorch] [winner of mot15,16,17,20]</li>
</ul>
<p><a id="siamese__1"></a></p>
<h3 id="siamese_1">Siamese<a class="headerlink" href="#siamese_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/amazon-research/siam-mot">SiamMOT: Siamese Multi-Object Tracking</a> [ax2105] [pytorch]</li>
</ul>
<p><a id="unsupervise_d_"></a></p>
<h3 id="unsupervised">Unsupervised<a class="headerlink" href="#unsupervised" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/zhen-he/tracking-by-animation">Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers</a> [cvpr19] [python/c++/pytorch]</li>
</ul>
<p><a id="re_id_"></a></p>
<h3 id="re-id">Re-ID<a class="headerlink" href="#re-id" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/KaiyangZhou/deep-person-reid">Torchreid: Deep learning person re-identification in PyTorch</a> [ax1910] [pytorch]</li>
<li><a href="https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo/smot">SMOT: Single-Shot Multi Object Tracking</a> [ax2010] [pytorch] [gluon-cv]</li>
<li><a href="https://github.com/ifzhang/FairMOT">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking</a> [ax2004] [pytorch] <a href="https://github.com/microsoft/FairMOT">[microsoft]</a> <a href="https://github.com/dingwoai/FairMOT-BDD100K">[BDD100K]</a> <a href="https://github.com/zengwb-lx/Face-Tracking-usingFairMOT">[face tracking]</a></li>
<li><a href="https://github.com/JudasDie/SOTS">Rethinking the competition between detection and ReID in Multi-Object Tracking</a> [ax2010] [pytorch] </li>
</ul>
<p><a id="framework_s__1"></a></p>
<h4 id="frameworks_1">Frameworks<a class="headerlink" href="#frameworks_1" title="Permanent link"></a></h4>
<ul>
<li><a href="https://github.com/open-mmlab/OpenUnReID">PyTorch open-source toolbox for unsupervised or domain adaptive object re-ID</a> [pytorch] </li>
</ul>
<p><a id="graph_nn_"></a></p>
<h3 id="graph-nn">Graph NN<a class="headerlink" href="#graph-nn" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/yongxinw/GSDT">Joint Object Detection and Multi-Object Tracking with Graph Neural Networks</a> [ax2006/ icra21] [pytorch]</li>
</ul>
<p><a id="microscopy___cell_tracking__1"></a></p>
<h3 id="microscopy-cell-tracking_1">Microscopy / cell tracking<a class="headerlink" href="#microscopy-cell-tracking_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/klasma/BaxterAlgorithms">Baxter Algorithms / Viterbi Tracking</a> [tmi14] [matlab]</li>
<li><a href="https://github.com/vanvalenlab/deepcell-tracking">Deepcell: Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning</a> [biorxiv1910] [tensorflow]</li>
</ul>
<p><a id="3_d_"></a></p>
<h3 id="3d">3D<a class="headerlink" href="#3d" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/xinshuoweng/AB3DMOT">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics </a> [iros20/eccvw20] [pytorch]</li>
<li><a href="https://github.com/xinshuoweng/GNN3DMOT">GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning </a> [iros20/eccvw20] [pytorch]</li>
</ul>
<p><a id="metrics__1"></a></p>
<h3 id="metrics_1">Metrics<a class="headerlink" href="#metrics_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/JonathonLuiten/HOTA-metrics">HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking</a> [cvpr20] [python]</li>
</ul>
<p><a id="single_object_tracking__2"></a></p>
<h2 id="single-object-tracking_2">Single Object Tracking<a class="headerlink" href="#single-object-tracking_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/zenhacker/TrackingAlgoCollection">A collection of common tracking algorithms (2003-2012)</a> [c++/matlab]</li>
<li><a href="https://github.com/STVIR/pysot/">SenseTime Research platform for single object tracking, implementing algorithms like SiamRPN and SiamMask</a> [pytorch]</li>
<li><a href="https://github.com/jbhuang0604/CF2">In Defense of Color-based Model-free Tracking</a> [cvpr15] [c++]</li>
<li><a href="https://github.com/jbhuang0604/CF2">Hierarchical Convolutional Features for Visual Tracking</a> [iccv15] [matlab]</li>
<li><a href="https://github.com/scott89/FCNT">Visual Tracking with Fully Convolutional Networks</a> [iccv15] [matlab]</li>
<li><a href="https://github.com/jbhuang0604/CF2">Hierarchical Convolutional Features for Visual Tracking</a> [iccv15] [matlab] </li>
<li><a href="https://github.com/pondruska/DeepTracking">DeepTracking: Seeing Beyond Seeing Using Recurrent Neural Networks</a> [aaai16] [torch 7]</li>
<li>Learning Multi-Domain Convolutional Neural Networks for Visual Tracking [cvpr16] [vot2015 winner] <a href="https://github.com/HyeonseobNam/MDNet">[matlab/matconvnet]</a> <a href="https://github.com/HyeonseobNam/py-MDNet">[pytorch]</a></li>
<li><a href="https://github.com/martin-danelljan/Continuous-ConvOp">Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</a> [eccv 2016] [matlab]</li>
<li><a href="https://github.com/bertinetto/siamese-fc">Fully-Convolutional Siamese Networks for Object Tracking</a> [eccvw 2016] [matlab/matconvnet] <a href="http://www.robots.ox.ac.uk/~luca/siamese-fc.html">[project]</a> <a href="https://github.com/huanglianghua/siamfc-pytorch">[pytorch]</a> <a href="https://github.com/rafellerc/Pytorch-SiamFC">[pytorch (only training)]</a></li>
<li><a href="https://arxiv.org/abs/1704.04057">DCFNet: Discriminant Correlation Filters Network for Visual Tracking</a> [ax1704] <a href="https://github.com/foolwood/DCFNet/">[matlab/matconvnet]</a> <a href="https://github.com/foolwood/DCFNet_pytorch/">[pytorch]</a></li>
<li><a href="https://arxiv.org/abs/1704.06036">End-to-end representation learning for Correlation Filter based tracking</a>
[cvpr17]
<a href="https://github.com/bertinetto/cfnet">[matlab/matconvnet]</a> <a href="https://github.com/torrvision/siamfc-tf">[tensorflow/inference_only]</a> <a href="http://www.robots.ox.ac.uk/~luca/siamese-fc.html">[project]</a></li>
<li><a href="https://github.com/chizhizhen/DNT">Dual Deep Network for Visual Tracking</a> [tip1704] [caffe]</li>
<li><a href="https://github.com/zllrunning/SiameseX.PyTorch">SiameseX: A simplified PyTorch implementation of Siamese networks for tracking: SiamFC, SiamRPN, SiamRPN++, SiamVGG, SiamDW, SiamRPN-VGG</a> [pytorch]</li>
<li><a href="https://github.com/saebrahimi/RATM">RATM: Recurrent Attentive Tracking Model</a> [cvprw17] [python]</li>
<li><a href="https://github.com/Guanghan/ROLO">ROLO : Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking</a> [iscas 2017] [tensorfow]</li>
<li><a href="https://arxiv.org/abs/1611.09224">ECO: Efficient Convolution Operators for Tracking</a>
[cvpr17]
<a href="https://github.com/martin-danelljan/ECO">[matlab]</a>
<a href="https://github.com/StrangerZhang/pyECO">[python/cuda]</a>
<a href="https://github.com/visionml/pytracking">[pytorch]</a></li>
<li><a href="https://github.com/ildoonet/tf-adnet-tracking">Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning</a> [cvpr17] [tensorflow]</li>
<li><a href="https://github.com/feichtenhofer/Detect-Track">Detect to Track and Track to Detect</a> [iccv17] [matlab]</li>
<li><a href="https://github.com/silverbottlep/meta_trackers">Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers</a> [eccv18] [pytorch]</li>
<li><a href="https://github.com/lifeng9472/STRCF">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</a> [cvpr18] [matlab]</li>
<li>High Performance Visual Tracking with Siamese Region Proposal Network [cvpr18] <a href="https://github.com/zkisthebest/Siamese-RPN">[pytorch/195]</a> <a href="https://github.com/songdejia/Siamese-RPN-pytorch">[pytorch/313]</a>  <a href="https://github.com/huanglianghua/siamrpn-pytorch">[pytorch/no_train/104]</a> <a href="https://github.com/HelloRicky123/Siamese-RPN">[pytorch/177]</a> </li>
<li><a href="https://github.com/foolwood/DaSiamRPN">Distractor-aware Siamese Networks for Visual Object Tracking</a> [eccv18] [vot18 winner] [pytorch]</li>
<li><a href="https://github.com/ybsong00/Vital_release">VITAL: VIsual Tracking via Adversarial Learning</a> [cvpr18] [matlab] <a href="https://github.com/abnerwang/py-Vital">[pytorch]</a> <a href="https://ybsong00.github.io/cvpr18_tracking/index.html">[project]</a></li>
<li><a href="https://github.com/foolwood/SiamMask">Fast Online Object Tracking and Segmentation: A Unifying Approach (SiamMask)</a> [cvpr19] [pytorch] <a href="http://www.robots.ox.ac.uk/~qwang/SiamMask/">[project]</a></li>
<li><a href="https://github.com/visionml/pytracking">PyTracking: A general python framework for training and running visual object trackers, based on PyTorch</a> [ECO/ATOM/DiMP/PrDiMP] [cvpr17/cvpr19/iccv19/cvpr20] [pytorch] </li>
<li><a href="https://github.com/594422814/UDT">Unsupervised Deep Tracking</a> [cvpr19] [matlab/matconvnet] <a href="https://github.com/594422814/UDT_pytorch">[pytorch]</a></li>
<li><a href="https://github.com/researchmm/SiamDW">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</a> [cvpr19] [pytorch]</li>
<li><a href="https://github.com/LPXTT/GradNet-Tensorflow">GradNet: Gradient-Guided Network for Visual Object Tracking</a> [iccv19] [tensorflow]</li>
<li><a href="https://github.com/iiau-tracker/SPLT">`Skimming-Perusal&rsquo; Tracking: A Framework for Real-Time and Robust Long-term Tracking</a> [iccv19] [tensorflow]</li>
<li><a href="https://github.com/vision4robotics/ARCF-tracker">Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking</a> [iccv19] [matlab]</li>
<li><a href="https://github.com/zhanglichao/updatenet">Learning the Model Update for Siamese Trackers</a> [iccv19] [pytorch]</li>
<li><a href="https://github.com/microsoft/SPM-Tracker">SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking</a> [cvpr19] [pytorch] [inference-only]</li>
<li><a href="https://github.com/XU-TIANYANG/GFS-DCF">Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking</a> [iccv19] [matlab]</li>
<li><a href="https://github.com/VisualComputingInstitute/SiamR-CNN">Siam R-CNN: Visual Tracking by Re-Detection</a> [cvpr20] [tensorflow]</li>
<li><a href="https://github.com/alanlukezic/d3s">D3S - Discriminative Single Shot Segmentation Tracker</a> [cvpr20] [pytorch/pytracking]</li>
<li><a href="https://github.com/shallowtoil/DROL">Discriminative and Robust Online Learning for Siamese Visual Tracking</a> [aaai20] [pytorch/pysot]</li>
<li><a href="https://github.com/hqucv/siamban">Siamese Box Adaptive Network for Visual Tracking</a> [cvpr20] [pytorch/pysot]</li>
<li><a href="https://github.com/JudasDie/SOTS">Ocean: Object-aware Anchor-free Tracking</a> [ax2010] [pytorch] </li>
</ul>
<p><a id="gui_application___large_scale_tracking___animal_s_"></a></p>
<h3 id="gui-application-large-scale-tracking-animals">GUI Application / Large Scale Tracking / Animals<a class="headerlink" href="#gui-application-large-scale-tracking-animals" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/BioroboticsLab/biotracker_core">BioTracker An Open-Source Computer Vision Framework for Visual Animal Tracking</a>[opencv/c++]</li>
<li><a href="https://github.com/vivekhsridhar/tracktor">Tracktor: Image‐based automated tracking of animal movement and behaviour</a>[opencv/c++]</li>
<li><a href="https://github.com/de-Bivort-Lab/margo">MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology</a>[matlab]</li>
<li><a href="https://gitlab.com/polavieja_lab/idtrackerai">idtracker.ai: Tracking all individuals in large collectives of unmarked animals</a>
[tensorflow]
<a href="https://idtracker.ai/">[project]</a></li>
</ul>
<p><a id="video_detectio_n__2"></a></p>
<h2 id="video-detection_2">Video Detection<a class="headerlink" href="#video-detection_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/msracver/Flow-Guided-Feature-Aggregation">Flow-Guided Feature Aggregation for Video Object Detection</a>
[nips16 / iccv17]
[mxnet]</li>
<li><a href="https://github.com/myfavouritekk/T-CNN">T-CNN: Tubelets with Convolution Neural Networks</a> [cvpr16] [python]  </li>
<li><a href="https://github.com/myfavouritekk/TPN">TPN: Tubelet Proposal Network</a> [cvpr17] [python]</li>
<li><a href="https://github.com/msracver/Deep-Feature-Flow">Deep Feature Flow for Video Recognition</a> [cvpr17] [mxnet]</li>
<li><a href="https://github.com/tensorflow/models/tree/master/research/lstm_object_detection">Mobile Video Object Detection with Temporally-Aware Feature Maps</a> [cvpr18] [Google] [tensorflow]  </li>
</ul>
<p><a id="action_detectio_n_"></a></p>
<h3 id="action-detection">Action Detection<a class="headerlink" href="#action-detection" title="Permanent link"></a></h3>
<p><a id="framework_s__2"></a></p>
<h4 id="frameworks_2">Frameworks<a class="headerlink" href="#frameworks_2" title="Permanent link"></a></h4>
<ul>
<li><a href="https://github.com/open-mmlab/mmaction2">OpenMMLab&rsquo;s Next Generation Video Understanding Toolbox and Benchmark</a> [pytorch]</li>
</ul>
<p><a id="static_detection_and_matching_"></a></p>
<h2 id="static-detection-and-matching">Static Detection and Matching<a class="headerlink" href="#static-detection-and-matching" title="Permanent link"></a></h2>
<p><a id="framework_s__3"></a></p>
<h3 id="frameworks_3">Frameworks<a class="headerlink" href="#frameworks_3" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/tensorflow/models/tree/master/object_detection">Tensorflow object detection API</a> [tensorflow]</li>
<li><a href="https://github.com/facebookresearch/detectron2">Detectron2</a> [pytorch]</li>
<li><a href="https://github.com/facebookresearch/Detectron">Detectron</a> [pytorch]</li>
<li><a href="https://github.com/open-mmlab/mmdetection">Open MMLab Detection Toolbox with PyTorch</a> [pytorch]</li>
<li><a href="https://github.com/tusen-ai/simpledet">SimpleDet</a> [mxnet]</li>
</ul>
<p><a id="region_proposal__1"></a></p>
<h3 id="region-proposal_1">Region Proposal<a class="headerlink" href="#region-proposal_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/jponttuset/mcg">MCG : Multiscale Combinatorial Grouping - Object Proposals and Segmentation</a>  <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">(project)</a> [tpami16/cvpr14] [python]</li>
<li><a href="https://github.com/kmaninis/COB">COB : Convolutional Oriented Boundaries</a>  <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/">(project)</a> [tpami18/eccv16] [matlab/caffe]</li>
</ul>
<p><a id="fpn_"></a></p>
<h3 id="fpn">FPN<a class="headerlink" href="#fpn" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/unsky/FPN">Feature Pyramid Networks for Object Detection</a> [caffe/python]  </li>
</ul>
<p><a id="rcn_n__1"></a></p>
<h3 id="rcnn_1">RCNN<a class="headerlink" href="#rcnn_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/daijifeng001/r-fcn">RFCN (author)</a> [caffe/matlab]</li>
<li><a href="https://github.com/xdever/RFCN-tensorflow">RFCN-tensorflow</a> [tensorflow]</li>
<li><a href="https://github.com/sanghoon/pva-faster-rcnn">PVANet: Lightweight Deep Neural Networks for Real-time Object Detection</a> [intel] [emdnn16(nips16)]</li>
<li>Mask R-CNN <a href="https://github.com/CharlesShang/FastMaskRCNN">[tensorflow]</a> <a href="https://github.com/matterport/Mask_RCNN">[keras]</a></li>
<li><a href="https://github.com/zengarden/light_head_rcnn">Light-head R-CNN</a> [cvpr18] [tensorflow]    </li>
<li><a href="https://github.com/Willy0919/Evolving_Boxes">Evolving Boxes for Fast Vehicle Detection</a> [icme18] [caffe/python]</li>
<li><a href="http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf">Cascade R-CNN (cvpr18)</a> <a href="https://github.com/zhaoweicai/Detectron-Cascade-RCNN">[detectron]</a> <a href="https://github.com/zhaoweicai/cascade-rcnn">[caffe]</a>  </li>
<li><a href="https://arxiv.org/abs/1604.02135">A MultiPath Network for Object Detection</a> <a href="https://github.com/facebookresearch/multipathnet">[torch]</a> [bmvc16] [facebook]</li>
<li><a href="https://github.com/mahyarnajibi/SNIPER">SNIPER: Efficient Multi-Scale Training/An Analysis of Scale Invariance in Object Detection-SNIP</a> [nips18/cvpr18] [mxnet]</li>
</ul>
<p><a id="ssd__1"></a></p>
<h3 id="ssd_1">SSD<a class="headerlink" href="#ssd_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/ljanyst/ssd-tensorflow">SSD-Tensorflow</a> [tensorflow]</li>
<li><a href="https://github.com/HiKapok/SSD.TensorFlow">SSD-Tensorflow (tf.estimator)</a> [tensorflow]</li>
<li><a href="https://github.com/balancap/SSD-Tensorflow">SSD-Tensorflow (tf.slim)</a> [tensorflow]</li>
<li><a href="https://github.com/rykov8/ssd_keras">SSD-Keras</a> [keras]</li>
<li><a href="https://github.com/amdegroot/ssd.pytorch">SSD-Pytorch</a> [pytorch]</li>
<li><a href="https://github.com/CVlengjiaxu/Enhanced-SSD-with-Feature-Fusion-and-Visual-Reasoning">Enhanced SSD with Feature Fusion and Visual Reasoning</a> [nca18] [tensorflow]</li>
<li><a href="https://github.com/sfzhang15/RefineDet">RefineDet - Single-Shot Refinement Neural Network for Object Detection</a> [cvpr18] [caffe]</li>
</ul>
<p><a id="retinanet__1"></a></p>
<h3 id="retinanet_1">RetinaNet<a class="headerlink" href="#retinanet_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/c0nn3r/RetinaNet">9.277.41</a> [pytorch]</li>
<li><a href="https://github.com/kuangliu/pytorch-retinanet">31.857.212</a> [pytorch]</li>
<li><a href="https://github.com/NVIDIA/retinanet-examples">25.274.84</a> [pytorch] [nvidia]</li>
<li><a href="https://github.com/yhenon/pytorch-retinanet">22.869.302</a> [pytorch]</li>
</ul>
<p><a id="yol_o__1"></a></p>
<h3 id="yolo_1">YOLO<a class="headerlink" href="#yolo_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/pjreddie/darknet">Darknet: Convolutional Neural Networks</a> [c/python]</li>
<li><a href="https://github.com/philipperemy/yolo-9000">YOLO9000: Better, Faster, Stronger - Real-Time Object Detection. 9000 classes!</a>  [c/python]</li>
<li><a href="https://github.com/thtrieu/darkflow">Darkflow</a> [tensorflow]</li>
<li><a href="https://github.com/marvis/pytorch-yolo2">Pytorch Yolov2 </a> [pytorch]</li>
<li><a href="https://github.com/AlexeyAB/darknet">Yolo-v3 and Yolo-v2 for Windows and Linux</a> [c/python]</li>
<li><a href="https://github.com/ultralytics/yolov3">YOLOv3 in PyTorch</a> [pytorch]</li>
<li><a href="https://github.com/ayooshkathuria/pytorch-yolo-v3">pytorch-yolo-v3 </a> [pytorch] [no training] <a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">[tutorial]</a></li>
<li><a href="https://github.com/wizyoung/YOLOv3_TensorFlow">YOLOv3_TensorFlow</a> [tensorflow]</li>
<li><a href="https://github.com/mystic123/tensorflow-yolo-v3">tensorflow-yolo-v3</a> [tensorflow slim]</li>
<li><a href="https://github.com/YunYang1994/tensorflow-yolov3">tensorflow-yolov3</a> [tensorflow slim]</li>
<li><a href="https://github.com/qqwweee/keras-yolo3">keras-yolov3</a> [keras]  </li>
<li>YOLOv4 <a href="https://github.com/AlexeyAB/darknet">[darknet - c/python]</a> <a href="https://github.com/hunglc007/tensorflow-yolov4-tflite">[tensorflow]</a> <a href="https://github.com/WongKinYiu/PyTorch_YOLOv4">[pytorch/711]</a> <a href="https://github.com/Tianxiaomo/pytorch-YOLOv4">[pytorch/ONNX/TensorRT/1.9k]</a> <a href="https://github.com/maudzung/Complex-YOLOv4-Pytorch">[pytorch 3D]</a></li>
<li><a href="https://github.com/ultralytics/yolov5">YOLOv5</a> [pytorch] </li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX">YOLOX</a> [pytorch] <a href="https://github.com/MegEngine/YOLOX">MegEngine</a> [ax2107]</li>
</ul>
<p><a id="anchor_free__1"></a></p>
<h3 id="anchor-free_1">Anchor Free<a class="headerlink" href="#anchor-free_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/taokong/FoveaBox">FoveaBox: Beyond Anchor-based Object Detector</a> [ax1904] [pytorch/mmdetection]</li>
<li><a href="https://github.com/princeton-vl/CornerNet">Cornernet: Detecting objects as paired keypoints</a> [ax1903/eccv18] [pytorch]</li>
<li><a href="https://github.com/tianzhi0549/FCOS">FCOS: Fully Convolutional One-Stage Object Detection</a> [iccv19] [pytorch] <a href="https://github.com/vov-net/VoVNet-FCOS">[VoVNet]</a> <a href="https://github.com/HRNet/HRNet-FCOS">[HRNet]</a> <a href="https://github.com/Lausannen/NAS-FCOS">[NAS]</a> <a href="https://github.com/yqyao/FCOS_PLUS">[FCOS_PLUS]</a></li>
<li><a href="https://github.com/hdjang/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a> [cvpr19] [pytorch]</li>
<li><a href="https://github.com/xingyizhou/CenterNet">CenterNet: Objects as Points</a> [ax1904] [pytorch]</li>
<li><a href="https://github.com/xingyizhou/ExtremeNet">Bottom-up Object Detection by Grouping Extreme and Center Points,</a> [cvpr19]  [pytorch]</li>
<li><a href="https://github.com/microsoft/RepPoints">RepPoints Point Set Representation for Object Detection</a> [iccv19]  [pytorch] [microsoft]</li>
<li><a href="https://github.com/facebookresearch/detr">DE⫶TR: End-to-End Object Detection with Transformers</a> [ax200528]  [pytorch] [facebook]</li>
<li><a href="https://github.com/sfzhang15/ATSS">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a> [cvpr20]  [pytorch]</li>
</ul>
<p><a id="mis_c__2"></a></p>
<h3 id="misc_2">Misc<a class="headerlink" href="#misc_2" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/msracver/Relation-Networks-for-Object-Detection">Relation Networks for Object Detection</a> [cvpr18] [mxnet]</li>
<li><a href="https://github.com/lachlants/denet">DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</a> [iccv17(poster)] [theano]</li>
<li><a href="https://github.com/Hwang64/MLKP">Multi-scale Location-aware Kernel Representation for Object Detection</a> [cvpr18]  [caffe/python]</li>
</ul>
<p><a id="matchin_g_"></a></p>
<h3 id="matching">Matching<a class="headerlink" href="#matching" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/hanxf/matchnet">Matchnet</a></li>
<li><a href="https://github.com/jzbontar/mc-cnn">Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches</a></li>
</ul>
<p><a id="boundary_detectio_n__1"></a></p>
<h3 id="boundary-detection_1">Boundary Detection<a class="headerlink" href="#boundary-detection_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/s9xie/hed">Holistically-Nested Edge Detection (HED) (iccv15)</a> [caffe]       </li>
<li><a href="https://github.com/Akuanchang/Edge-Detection-using-Deep-Learning">Edge-Detection-using-Deep-Learning (HED)</a> [tensorflow]</li>
<li><a href="https://github.com/opencv/opencv/blob/master/samples/dnn/edge_detection.py">Holistically-Nested Edge Detection (HED) in OpenCV</a> [python/c++]       </li>
<li><a href="https://github.com/phillipi/crisp-boundaries">Crisp Boundary Detection Using Pointwise Mutual Information (eccv14)</a> [matlab]</li>
<li><a href="https://github.com/phillipi/crisp-boundaries">Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection</a> [wacv20] <a href="https://github.com/xavysp/DexiNed/tree/master/legacy">tensorflow</a> <a href="https://github.com/xavysp/DexiNed">pytorch</a></li>
</ul>
<p><a id="text_detectio_n_"></a></p>
<h3 id="text-detection">Text Detection<a class="headerlink" href="#text-detection" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/MhLiao/DB">Real-time Scene Text Detection with Differentiable Binarization</a> [pytorch] [aaai20] </li>
</ul>
<p><a id="framework_s__4"></a></p>
<h4 id="frameworks_4">Frameworks<a class="headerlink" href="#frameworks_4" title="Permanent link"></a></h4>
<ul>
<li><a href="https://github.com/open-mmlab/mmocr"> OpenMMLab Text Detection, Recognition and Understanding Toolbox </a> [pytorch]</li>
</ul>
<p><a id="3d_detectio_n_"></a></p>
<h3 id="3d-detection">3D Detection<a class="headerlink" href="#3d-detection" title="Permanent link"></a></h3>
<p><a id="framework_s__5"></a></p>
<h4 id="frameworks_5">Frameworks<a class="headerlink" href="#frameworks_5" title="Permanent link"></a></h4>
<ul>
<li><a href="https://github.com/open-mmlab/mmdetection3d">OpenMMLab&rsquo;s next-generation platform for general 3D object detection</a> [pytorch]</li>
<li><a href="https://github.com/open-mmlab/OpenPCDet">OpenPCDet Toolbox for LiDAR-based 3D Object Detection</a> [pytorch]</li>
</ul>
<p><a id="optical_flow__1"></a></p>
<h2 id="optical-flow_1">Optical Flow<a class="headerlink" href="#optical-flow_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (cvpr17)</a> <a href="https://github.com/lmb-freiburg/flownet2">[caffe]</a> <a href="https://github.com/NVIDIA/flownet2-pytorch">[pytorch/nvidia]</a></li>
<li><a href="https://arxiv.org/abs/1702.02295">SPyNet: Spatial Pyramid Network for Optical Flow (cvpr17)</a> <a href="https://github.com/anuragranj/spynet">[lua]</a> <a href="https://github.com/sniklaus/pytorch-spynet">[pytorch]</a></li>
<li><a href="https://arxiv.org/abs/1702.02295">Guided Optical Flow Learning (cvprw17)</a> <a href="https://github.com/bryanyzhu/GuidedNet">[caffe]</a> <a href="https://github.com/bryanyzhu/deepOF">[tensorflow]</a></li>
<li><a href="https://github.com/tikroeger/OF_DIS">Fast Optical Flow using Dense Inverse Search (DIS)</a> [eccv16] [C++]</li>
<li><a href="https://github.com/jadarve/optical-flow-filter">A Filter Formulation for Computing Real Time Optical Flow</a> [ral16] [c++/cuda - matlab,python wrappers]</li>
<li><a href="https://github.com/DediGadot/PatchBatch">PatchBatch - a Batch Augmented Loss for Optical Flow</a> [cvpr16] [python/theano]</li>
<li><a href="https://github.com/vogechri/PRSM">Piecewise Rigid Scene Flow</a> [iccv13/eccv14/ijcv15] [c++/matlab]</li>
<li><a href="https://arxiv.org/abs/1611.00850">DeepFlow v2</a> [iccv13] <a href="https://github.com/zimenglan-sysu-512/deep-flow">[c++/python/matlab]</a>, <a href="http://lear.inrialpes.fr/src/deepflow/">[project]</a></li>
<li><a href="https://github.com/vogechri/DataFlow">An Evaluation of Data Costs for Optical Flow</a> [gcpr13] [matlab]</li>
</ul>
<p><a id="framework_s__6"></a></p>
<h3 id="frameworks_6">Frameworks<a class="headerlink" href="#frameworks_6" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmflow">OpenMMLab optical flow toolbox and benchmark </a> [pytorch]</li>
</ul>
<p><a id="instance_segmentation_"></a></p>
<h2 id="instance-segmentation">Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/msracver/FCIS">Fully Convolutional Instance-aware Semantic Segmentation</a> [cvpr17] [coco16 winner] [mxnet]</li>
<li><a href="https://github.com/daijifeng001/MNC">Instance-aware Semantic Segmentation via Multi-task Network Cascades</a> [cvpr16] [caffe] [coco15 winner]    </li>
<li><a href="https://arxiv.org/abs/1603.08695">DeepMask/SharpMask</a> [nips15/eccv16] [facebook] <a href="https://github.com/facebookresearch/deepmask">[torch]</a> <a href="https://github.com/aby2s/sharpmask">[tensorflow]</a>  <a href="https://github.com/foolwood/deepmask-pytorch/">[pytorch/deepmask]</a> </li>
<li><a href="https://github.com/bharath272/sds_eccv2014">Simultaneous Detection and Segmentation</a> [eccv14] [matlab] <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/">[project]</a>    </li>
<li><a href="https://github.com/ShuLiu1993/PANet">PANet</a> [cvpr18] [pytorch]</li>
<li><a href="https://github.com/chengyangfu/retinamask">RetinaMask</a> [arxviv1901] [pytorch]</li>
<li><a href="https://github.com/zjhuang22/maskscoring_rcnn">Mask Scoring R-CNN</a> [cvpr19] [pytorch]</li>
<li><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/deepmac.md">DeepMAC</a> [ax2104] [tensorflow]</li>
<li><a href="https://github.com/microsoft/Swin-Transformer">Swin Transformer</a> [iccv21] [pytorch] [microsoft]</li>
</ul>
<p><a id="framework_s__7"></a></p>
<h3 id="frameworks_7">Frameworks<a class="headerlink" href="#frameworks_7" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/facebookresearch/maskrcnn-benchmark">Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</a> [pytorch] [facebook]</li>
<li><a href="https://github.com/PaddlePaddle/PaddleDetection">PaddleDetection, Object detection and instance segmentation toolkit based on PaddlePaddle.</a> [2019]</li>
</ul>
<p><a id="semantic_segmentation_"></a></p>
<h2 id="semantic-segmentation">Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/swamiviv/LSD-seg">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</a> [cvpr18] [spotlight] [pytorch]</li>
<li><a href="https://github.com/shelhamer/revolver">Few-shot Segmentation Propagation with Guided Networks</a> [ax1806] [pytorch] [incomplete]</li>
<li><a href="https://github.com/speedinghzl/pytorch-segmentation-toolbox">Pytorch-segmentation-toolbox</a> [DeeplabV3 and PSPNet] [pytorch]</li>
<li><a href="https://github.com/tensorflow/models/tree/master/research/deeplab">DeepLab</a> [tensorflow]</li>
<li><a href="https://github.com/MenghaoGuo/AutoDeeplab">Auto-DeepLab</a> [pytorch]</li>
<li><a href="https://github.com/jfzhang95/pytorch-deeplab-xception">DeepLab v3+</a> [pytorch]</li>
<li><a href="https://github.com/scaelles/DEXTR-PyTorch">Deep Extreme Cut (DEXTR): From Extreme Points to Object Segmentation</a>[cvpr18]<a href="https://cvlsegmentation.github.io/dextr/">[project]</a> [pytorch]</li>
<li><a href="https://github.com/wuhuikai/FastFCN">FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</a>[ax1903]<a href="http://wuhuikai.me/FastFCNProject/">[project]</a> [pytorch]</li>
</ul>
<p><a id="framework_s__8"></a></p>
<h3 id="frameworks_8">Frameworks<a class="headerlink" href="#frameworks_8" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmsegmentation"> OpenMMLab Semantic Segmentation Toolbox and Benchmark</a> [pytorch]</li>
</ul>
<p><a id="polyp_"></a></p>
<h3 id="polyp">Polyp<a class="headerlink" href="#polyp" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/DengPingFan/PraNet">PraNet: Parallel Reverse Attention Network for Polyp Segmentation</a>[miccai20]</li>
<li><a href="https://github.com/james128333/HarDNet-MSEG">PHarDNet-MSEG: A Simple Encoder-Decoder Polyp Segmentation Neural Network that Achieves over 0.9 Mean Dice and 86 FPS</a>[ax2101]</li>
</ul>
<p><a id="panoptic_segmentation_"></a></p>
<h2 id="panoptic-segmentation">Panoptic Segmentation<a class="headerlink" href="#panoptic-segmentation" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/facebookresearch/detectron2/tree/main/projects/Panoptic-DeepLab">Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation</a> [cvpr20] [pytorch]</li>
</ul>
<p><a id="video_segmentation__1"></a></p>
<h2 id="video-segmentation_1">Video Segmentation<a class="headerlink" href="#video-segmentation_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/NVIDIA/semantic-segmentation">Improving Semantic Segmentation via Video Prediction and Label Relaxation</a> [cvpr19] [pytorch] [nvidia]</li>
<li><a href="https://github.com/JonathonLuiten/PReMVOS">PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation</a> [accv18/cvprw18/eccvw18] [tensorflow]</li>
<li><a href="https://github.com/youtubevos/MaskTrackRCNN">MaskTrackRCNN for video instance segmentation</a> [iccv19] [pytorch/detectron]</li>
<li><a href="https://github.com/youtubevos/MaskTrackRCNN">MaskTrackRCNN</a> [iccv19] [pytorch/detectron]</li>
<li><a href="https://github.com/sukjunhwang/IFC">Video Instance Segmentation using Inter-Frame Communication Transformers</a> [nips21] [pytorch/detectron]</li>
<li><a href="https://github.com/wjf5203/VNext">VNext: SeqFormer / IDOL</a> [eccv22] [pytorch/detectron2]</li>
<li><a href="https://github.com/wjf5203/SeqFormer">SeqFormer: Sequential Transformer for Video Instance Segmentation</a> [eccv22] [pytorch/detectron2]</li>
<li><a href="https://github.com/sukjunhwang/vita">VITA: Video Instance Segmentation via Object Token Association</a> [nips22] [pytorch/detectron2]</li>
</ul>
<p><a id="panoptic_video_segmentation_"></a></p>
<h3 id="panoptic-video-segmentation">Panoptic Video Segmentation<a class="headerlink" href="#panoptic-video-segmentation" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/joe-siyuan-qiao/ViP-DeepLab">ViP-DeepLab</a> [cvpr21] </li>
</ul>
<p><a id="motion_prediction__1"></a></p>
<h2 id="motion-prediction_1">Motion Prediction<a class="headerlink" href="#motion-prediction_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/XiaohangZhan/conditional-motion-propagation">Self-Supervised Learning via Conditional Motion Propagation</a> [cvpr19] [pytorch]</li>
<li><a href="https://github.com/cr7anand/neural_temporal_models">A Neural Temporal Model for Human Motion Prediction</a> [cvpr19] [tensorflow]   </li>
<li><a href="https://github.com/wei-mao-2019/LearnTrajDep">Learning Trajectory Dependencies for Human Motion Prediction</a> [iccv19] [pytorch]   </li>
<li><a href="https://github.com/zhaolongkzz/human_motion">Structural-RNN: Deep Learning on Spatio-Temporal Graphs</a> [cvpr15] [tensorflow]   </li>
<li><a href="https://github.com/MarlonCajamarca/Keras-LSTM-Trajectory-Prediction">A Keras multi-input multi-output LSTM-based RNN for object trajectory forecasting</a> [keras]   </li>
<li><a href="https://github.com/FGiuliari/Trajectory-Transformer">Transformer Networks for Trajectory Forecasting</a> [ax2003] [pytorch]  </li>
<li><a href="https://github.com/d1024choi/traj-pred-irl">Regularizing neural networks for future trajectory prediction via IRL framework</a> [ietcv1907] [tensorflow]  </li>
<li><a href="https://github.com/JunweiLiang/next-prediction">Peeking into the Future: Predicting Future Person Activities and Locations in Videos</a> [cvpr19] [tensorflow]  </li>
<li><a href="https://github.com/alexmonti19/dagnet">DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting</a> [ax200526] [pytorch]  </li>
<li><a href="https://github.com/sugmichaelyang/MCENET">MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory Prediction in Mixed Traffic</a> [ax200405] [tensorflow]  </li>
<li><a href="https://github.com/biy001/social-cnn-pytorch">Human Trajectory Prediction in Socially Interacting Crowds Using a CNN-based Architecture</a> [pytorch]  </li>
<li><a href="https://github.com/xuehaouwa/Trajectory-Prediction-Tools">A tool set for trajectory prediction, ready for pip install</a> [icai19/wacv19]  [pytorch]  </li>
<li><a href="https://github.com/xuehaouwa/Trajectory-Prediction-Tools">RobustTP: End-to-End Trajectory Prediction for Heterogeneous Road-Agents in Dense Traffic with Noisy Sensor Inputs</a> [acmcscs19]  [pytorch/tensorflow]  </li>
<li><a href="https://github.com/JunweiLiang/Multiverse">The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction</a> [cvpr20] [dummy] </li>
<li><a href="https://github.com/lmb-freiburg/Multimodal-Future-Prediction">Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction</a> [cvpr19] [tensorflow] </li>
<li><a href="https://github.com/vita-epfl/AdversarialLoss-SGAN">Adversarial Loss for Human Trajectory Prediction</a> [hEART19] [pytorch] </li>
<li><a href="https://github.com/agrimgupta92/sgan">Social GAN: SSocially Acceptable Trajectories with Generative Adversarial Networks</a> [cvpr18] [pytorch] </li>
<li><a href="https://github.com/rohanchandra30/Spectral-Trajectory-and-Behavior-Prediction">Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs</a> [ax1912] [pytorch] </li>
<li><a href="https://github.com/xuehaouwa/Trajectory-Prediction-Tools">Study of attention mechanisms for trajectory prediction in Deep Learning</a> [msc thesis]  [python]  </li>
<li><a href="https://github.com/chrisHuxi/Trajectory_Predictor">A python implementation of multi-model estimation algorithm for trajectory tracking and prediction, research project from BMW ABSOLUT self-driving bus project.</a> [python]  </li>
<li><a href="https://github.com/karthik4444/nn-trajectory-prediction">Prediciting Human Trajectories</a> [theano]  </li>
<li><a href="https://github.com/aroongta/Pedestrian_Trajectory_Prediction">Implementation of Recurrent Neural Networks for future trajectory prediction of pedestrians</a> [pytorch]  </li>
</ul>
<p><a id="pose_estimation_"></a></p>
<h2 id="pose-estimation">Pose Estimation<a class="headerlink" href="#pose-estimation" title="Permanent link"></a></h2>
<p><a id="framework_s__9"></a></p>
<h3 id="frameworks_9">Frameworks<a class="headerlink" href="#frameworks_9" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmpose">OpenMMLab Pose Estimation Toolbox and Benchmark. </a> [pytorch]</li>
</ul>
<p><a id="autoencoder_s_"></a></p>
<h2 id="autoencoders">Autoencoders<a class="headerlink" href="#autoencoders" title="Permanent link"></a></h2>
<ul>
<li><a href="https://openreview.net/forum?id=Sy2fzU9gl">β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a> [iclr17] [deepmind] <a href="https://github.com/miyosuda/disentangled_vae">[tensorflow]</a> <a href="https://github.com/LynnHo/VAE-Tensorflow">[tensorflow]</a> <a href="https://github.com/1Konny/Beta-VAE">[pytorch]</a></li>
<li><a href="https://github.com/1Konny/FactorVAE">Disentangling by Factorising</a> [ax1806] [pytorch]   </li>
</ul>
<p><a id="classificatio_n__1"></a></p>
<h2 id="classification_1">Classification<a class="headerlink" href="#classification_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/miyosuda/async_deep_reinforce">Learning Efficient Convolutional Networks Through Network Slimming</a> [iccv17] [pytorch]</li>
</ul>
<p><a id="framework_s__10"></a></p>
<h3 id="frameworks_10">Frameworks<a class="headerlink" href="#frameworks_10" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmclassification">OpenMMLab Image Classification Toolbox and Benchmark</a> [pytorch]</li>
</ul>
<p><a id="deep_rl_"></a></p>
<h2 id="deep-rl">Deep RL<a class="headerlink" href="#deep-rl" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/miyosuda/async_deep_reinforce">Asynchronous Methods for Deep Reinforcement Learning </a></li>
</ul>
<p><a id="annotatio_n_"></a></p>
<h2 id="annotation">Annotation<a class="headerlink" href="#annotation" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/tzutalin/labelImg">LabelImg</a></li>
<li><a href="https://github.com/NathanUA/ByLabel">ByLabel: A Boundary Based Semi-Automatic Image Annotation Tool</a></li>
<li><a href="https://github.com/persts/BBoxEE">Bounding Box Editor and Exporter</a></li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/software/via/">VGG Image Annotator</a></li>
<li><a href="https://github.com/Microsoft/VoTT">Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos</a></li>
<li><a href="https://github.com/abreheret/PixelAnnotationTool">PixelAnnotationTool</a></li>
<li><a href="https://github.com/wkentaro/labelme">labelme : Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation)</a></li>
<li><a href="https://github.com/cvondrick/vatic">VATIC - Video Annotation Tool from Irvine, California)</a> [ijcv12] <a href="http://www.cs.columbia.edu/~vondrick/vatic/">[project]</a></li>
<li><a href="https://github.com/opencv/cvat">Computer Vision Annotation Tool (CVAT)</a></li>
<li><a href="https://bitbucket.org/ueacomputervision/image-labelling-tool/">Image labelling tool</a></li>
<li><a href="https://github.com/Labelbox/Labelbox">Labelbox</a> [paid]</li>
<li><a href="https://rectlabel.com/">RectLabel An image annotation tool to label images for bounding box object detection and segmentation.</a> [paid]</li>
<li><a href="https://github.com/onepanelio/core">Onepanel: Production scale vision AI platform with fully integrated components for model building, automated labeling, data processing and model training pipelines.</a> <a href="https://docs.onepanel.ai/docs/getting-started/quickstart/">[docs]</a></li>
</ul>
<p><a id="editing_"></a></p>
<h3 id="editing">Editing<a class="headerlink" href="#editing" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmediting">OpenMMLab Image and Video Editing Toolbox</a></li>
</ul>
<p><a id="augmentatio_n_"></a></p>
<h3 id="augmentation">Augmentation<a class="headerlink" href="#augmentation" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/mdbloice/Augmentor">Augmentor: Image augmentation library in Python for machine learning</a></li>
<li><a href="https://github.com/albumentations-team/albumentations">Albumentations: Fast image augmentation library and easy to use wrapper around other libraries</a></li>
<li><a href="https://github.com/aleju/imgaug">imgaug: Image augmentation for machine learning experiments</a></li>
<li><a href="https://github.com/MIPT-Oulu/solt">solt: Image Streaming over lightweight data transformations</a></li>
</ul>
<p><a id="deep_learning__2"></a></p>
<h2 id="deep-learning_2">Deep Learning<a class="headerlink" href="#deep-learning_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/msracver/Deformable-ConvNets">Deformable Convolutional Networks</a></li>
<li><a href="https://github.com/asheshjain399/RNNexp">RNNexp</a></li>
<li><a href="https://github.com/ramprs/grad-cam/">Grad-CAM: Gradient-weighted Class Activation Mapping</a></li>
</ul>
<p><a id="class_imbalanc_e_"></a></p>
<h3 id="class-imbalance">Class Imbalance<a class="headerlink" href="#class-imbalance" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/ufoym/imbalanced-dataset-sampler">Imbalanced Dataset Sampler</a> [pytorch]</li>
<li><a href="https://github.com/MaxHalford/pytorch-resample">Iterable dataset resampling in PyTorch</a> [pytorch]</li>
</ul>
<p><a id="few_shot_learning_"></a></p>
<h3 id="few-shot-learning">Few shot learning<a class="headerlink" href="#few-shot-learning" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/mmfewshot">OpenMMLab FewShot Learning Toolbox and Benchmark</a> [pytorch]</li>
</ul>
<p><a id="unsupervised_learning__2"></a></p>
<h3 id="unsupervised-learning_2">Unsupervised learning<a class="headerlink" href="#unsupervised-learning_2" title="Permanent link"></a></h3>
<ul>
<li><a href="https://github.com/open-mmlab/OpenSelfSup">Self-Supervised Learning Toolbox and Benchmark</a> [pytorch]</li>
</ul>
<p><a id="collections_"></a></p>
<h1 id="collections">Collections<a class="headerlink" href="#collections" title="Permanent link"></a></h1>
<p><a id="dataset_s__1"></a></p>
<h2 id="datasets_1">Datasets<a class="headerlink" href="#datasets_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/awesomedata/awesome-public-datasets">Awesome Public Datasets</a> </li>
<li><a href="https://github.com/gustavovelascoh/traffic-surveillance-dataset">List of traffic surveillance datasets</a> </li>
<li><a href="https://www.datasetlist.com/">Machine learning datasets: A list of the biggest machine learning datasets from across the web</a> </li>
<li><a href="http://lila.science/datasets">Labeled Information Library of Alexandria: Biology and Conservation</a> <a href="http://lila.science/otherdatasets">[other conservation data sets]</a> </li>
<li><a href="https://thoth.inrialpes.fr/data">THOTH: Data Sets &amp; Images</a> </li>
<li><a href="https://ai.google/tools/datasets/">Google AI Datasets</a> </li>
<li><a href="https://cloud.google.com/storage/docs/public-datasets/">Google Cloud Storage public datasets</a> </li>
<li><a href="https://msropendata.com/">Microsoft Research Open Data</a> </li>
<li><a href="https://developers.google.com/earth-engine/datasets/catalog/">Earth Engine Data Catalog</a> </li>
<li><a href="https://registry.opendata.aws/">Registry of Open Data on AWS</a> </li>
<li><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a> </li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm">CVonline: Image Databases</a> </li>
<li><a href="https://github.com/unrealcv/synthetic-computer-vision">Synthetic for Computer Vision: A list of synthetic dataset and tools for computer vision</a> </li>
<li><a href="https://pgram.com/category/vision/">pgram machine learning datasets</a> </li>
<li><a href="https://pgram.com/">pgram vision datasets</a> </li>
</ul>
<p><a id="deep_learning__3"></a></p>
<h2 id="deep-learning_3">Deep Learning<a class="headerlink" href="#deep-learning_3" title="Permanent link"></a></h2>
<ul>
<li><a href="https://modelzoo.co/">Model Zoo : Discover open source deep learning code and pretrained models</a></li>
</ul>
<p><a id="static_detectio_n__2"></a></p>
<h2 id="static-detection_2">Static Detection<a class="headerlink" href="#static-detection_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html">Object Detection with Deep Learning</a></li>
</ul>
<p><a id="video_detectio_n__3"></a></p>
<h2 id="video-detection_3">Video Detection<a class="headerlink" href="#video-detection_3" title="Permanent link"></a></h2>
<ul>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#video-object-detection">Video Object Detection with Deep Learning</a></li>
</ul>
<p><a id="single_object_tracking__3"></a></p>
<h2 id="single-object-tracking_3">Single Object Tracking<a class="headerlink" href="#single-object-tracking_3" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/foolwood/benchmark_results">Visual Tracking Paper List</a></li>
<li><a href="https://github.com/handong1587/handong1587.github.io/blob/master/_posts/deep_learning/2015-10-09-tracking.md">List of deep learning based tracking papers</a></li>
<li><a href="https://github.com/foolwood/benchmark_results">List of single object trackers with results on OTB</a></li>
<li><a href="https://github.com/lukaswals/cf-trackers">Collection of Correlation Filter based trackers with links to papers, codes, etc</a></li>
<li><a href="http://www.votchallenge.net/vot2018/trackers.html">VOT2018 Trackers repository</a></li>
<li><a href="http://mmlab.ie.cuhk.edu.hk.login.ezproxy.library.ualberta.ca/datasets.html">CUHK Datasets</a></li>
<li><a href="https://linkinpark213.com/2019/06/11/cvpr19-track/">A Summary of CVPR19 Visual Tracking Papers</a></li>
<li><a href="https://github.com/czla/daily-paper-visual-tracking">Visual Trackers for Single Object</a></li>
</ul>
<p><a id="multi_object_tracking__3"></a></p>
<h2 id="multi-object-tracking_3">Multi Object Tracking<a class="headerlink" href="#multi-object-tracking_3" title="Permanent link"></a></h2>
<ul>
<li><a href="http://perception.yale.edu/Brian/refGuides/MOT.html">List of multi object tracking papers</a>   </li>
<li><a href="https://github.com/huanglianghua/mot-papers">A collection of Multiple Object Tracking (MOT) papers in recent years, with notes</a>  </li>
<li><a href="https://paperswithcode.com/task/multiple-object-tracking/codeless">Papers with Code : Multiple Object Tracking</a>  </li>
<li><a href="https://github.com/SpyderXu/multi-object-tracking-paper-list">Paper list and source code for multi-object-tracking</a>  </li>
</ul>
<p><a id="static_segmentation__1"></a></p>
<h2 id="static-segmentation_1">Static Segmentation<a class="headerlink" href="#static-segmentation_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/segmentation.html">Segmentation Papers and Code</a>  </li>
<li><a href="https://github.com/wutianyiRosun/Segmentation.X">Segmentation.X : Papers and Benchmarks about semantic segmentation, instance segmentation, panoptic segmentation and video segmentation</a> </li>
<li><a href="https://paperswithcode.com/task/instance-segmentation">Instance Segmentation Papers with Code</a> </li>
</ul>
<p><a id="video_segmentation__2"></a></p>
<h2 id="video-segmentation_2">Video Segmentation<a class="headerlink" href="#video-segmentation_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?p=seqformer-a-frustratingly-simple-model-for">Video Instance Segmentation on YouTube-VIS validation</a></li>
</ul>
<p><a id="motion_prediction__2"></a></p>
<h2 id="motion-prediction_2">Motion Prediction<a class="headerlink" href="#motion-prediction_2" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/xuehaouwa/Awesome-Trajectory-Prediction/blob/master/README.md">Awesome-Trajectory-Prediction</a>  </li>
<li><a href="https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction">Awesome Interaction-aware Behavior and Trajectory Prediction</a>  </li>
<li><a href="https://github.com/amiryanj/OpenTraj">Human Trajectory Prediction Datasets</a>  </li>
</ul>
<p><a id="deep_compressed_sensin_g_"></a></p>
<h2 id="deep-compressed-sensing">Deep Compressed Sensing<a class="headerlink" href="#deep-compressed-sensing" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/AtenaKid/Reproducible-Deep-Compressive-Sensing">Reproducible Deep Compressive Sensing</a>  </li>
</ul>
<p><a id="mis_c__3"></a></p>
<h2 id="misc_3">Misc<a class="headerlink" href="#misc_3" title="Permanent link"></a></h2>
<ul>
<li><a href="https://paperswithcode.com/">Papers With Code : the latest in machine learning</a></li>
<li><a href="https://github.com/patrickcgray/awesome-deep-ecology">Awesome Deep Ecology</a></li>
<li><a href="https://github.com/uhub/awesome-matlab">List of Matlab frameworks, libraries and software</a></li>
<li><a href="https://github.com/ChanChiChoi/awesome-Face_Recognition">Face Recognition</a></li>
<li><a href="https://medium.com/@hyponymous/a-month-of-machine-learning-paper-summaries-ddd4dcf6cfa5">A Month of Machine Learning Paper Summaries</a></li>
<li><a href="https://github.com/memoiry/Awesome-model-compression-and-acceleration/blob/master/README.md">Awesome-model-compression-and-acceleration</a></li>
<li><a href="https://github.com/chester256/Model-Compression-Papers">Model-Compression-Papers</a></li>
</ul>
<p><a id="tutorials_"></a></p>
<h1 id="tutorials">Tutorials<a class="headerlink" href="#tutorials" title="Permanent link"></a></h1>
<p><a id="collections__1"></a></p>
<h2 id="collections_1">Collections<a class="headerlink" href="#collections_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://github.com/sgrvinod/Deep-Tutorials-for-PyTorch">Deep Tutorials for PyTorch</a></li>
</ul>
<p><a id="multi_object_tracking__4"></a></p>
<h2 id="multi-object-tracking_4">Multi Object Tracking<a class="headerlink" href="#multi-object-tracking_4" title="Permanent link"></a></h2>
<ul>
<li><a href="https://deepomatic.com/en/moving-beyond-deepomatic-learns-how-to-track-multiple-objects/">What is the Multi-Object Tracking (MOT) system?</a></li>
</ul>
<p><a id="static_detectio_n__3"></a></p>
<h2 id="static-detection_3">Static Detection<a class="headerlink" href="#static-detection_3" title="Permanent link"></a></h2>
<ul>
<li><a href="https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers">End-to-end object detection with Transformers</a></li>
<li><a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">Deep Learning for Object Detection: A Comprehensive Review</a></li>
<li><a href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852">Review of Deep Learning Algorithms for Object Detection</a>  </li>
<li><a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">A Simple Guide to the Versions of the Inception Network</a>    </li>
<li><a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">R-CNN, Fast R-CNN, Faster R-CNN, YOLO - Object Detection Algorithms</a></li>
<li><a href="https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/">A gentle guide to deep learning object detection</a></li>
<li><a href="https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d">The intuition behind RetinaNet</a></li>
<li><a href="https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006">YOLO—You only look once, real time object detection explained</a></li>
<li><a href="https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c">Understanding Feature Pyramid Networks for object detection (FPN)</a></li>
<li><a href="https://medium.com/omnius/fast-object-detection-with-squeezedet-on-keras-5cdd124b46ce">Fast object detection with SqueezeDet on Keras</a></li>
<li><a href="https://deepsense.ai/region-of-interest-pooling-explained/">Region of interest pooling explained</a></li>
</ul>
<p><a id="video_detectio_n__4"></a></p>
<h2 id="video-detection_4">Video Detection<a class="headerlink" href="#video-detection_4" title="Permanent link"></a></h2>
<ul>
<li><a href="https://medium.com/nurture-ai/how-microsoft-does-video-object-detection-unifying-the-best-techniques-in-video-object-detection-b78b63e3f1d8">How Microsoft Does Video Object Detection - Unifying the Best Techniques in Video Object Detection Architectures in a Single Model</a></li>
</ul>
<p><a id="instance_segmentation__1"></a></p>
<h2 id="instance-segmentation_1">Instance Segmentation<a class="headerlink" href="#instance-segmentation_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46">Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow</a></li>
<li><a href="https://medium.com/@alittlepain833/simple-understanding-of-mask-rcnn-134b5b330e95">Simple Understanding of Mask RCNN</a></li>
<li><a href="https://research.fb.com/blog/2016/08/learning-to-segment/">Learning to Segment</a></li>
<li><a href="https://adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook's-Computer-Vision-Approach/">Analyzing The Papers Behind Facebook&rsquo;s Computer Vision Approach</a></li>
<li><a href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">Review: MNC — Multi-task Network Cascade, Winner in 2015 COCO Segmentation</a></li>
<li><a href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">Review: FCIS — Winner in 2016 COCO Segmentation</a></li>
<li><a href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Review: InstanceFCN — Instance-Sensitive Score Maps</a></li>
</ul>
<p><a id="deep_learning__4"></a></p>
<h2 id="deep-learning_4">Deep Learning<a class="headerlink" href="#deep-learning_4" title="Permanent link"></a></h2>
<p><a id="optimizatio_n_"></a></p>
<h3 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link"></a></h3>
<ul>
<li><a href="https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/">Learning Rate Scheduling</a></li>
</ul>
<p><a id="class_imbalanc_e__1"></a></p>
<h3 id="class-imbalance_1">Class Imbalance<a class="headerlink" href="#class-imbalance_1" title="Permanent link"></a></h3>
<ul>
<li><a href="https://www.jeremyjordan.me/imbalanced-data/">Learning from imbalanced data</a></li>
<li><a href="https://www.svds.com/learning-imbalanced-classes/">Learning from Imbalanced Classes</a></li>
<li><a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28">Handling imbalanced datasets in machine learning</a> [medium]</li>
<li><a href="https://medium.com/quantyca/how-to-handle-class-imbalance-problem-9ee3062f2499">How to handle Class Imbalance Problem</a> [medium]</li>
<li><a href="https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18">Dealing with Imbalanced Data</a> [towardsdatascience]</li>
<li><a href="https://elitedatascience.com/imbalanced-classes">How to Handle Imbalanced Classes in Machine Learning</a> [elitedatascience]</li>
<li><a href="https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html">7 Techniques to Handle Imbalanced Data</a> [kdnuggets]</li>
<li><a href="https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/">10 Techniques to deal with Imbalanced Classes in Machine Learning</a> [analyticsvidhya]</li>
</ul>
<p><a id="rnn__2"></a></p>
<h2 id="rnn_2">RNN<a class="headerlink" href="#rnn_2" title="Permanent link"></a></h2>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
</ul>
<p><a id="deep_rl__1"></a></p>
<h2 id="deep-rl_1">Deep RL<a class="headerlink" href="#deep-rl_1" title="Permanent link"></a></h2>
<ul>
<li><a href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from Pixels</a></li>
<li><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
</ul>
<p><a id="autoencoder_s__1"></a></p>
<h2 id="autoencoders_1">Autoencoders<a class="headerlink" href="#autoencoders_1" title="Permanent link"></a></h2>
<ul>
<li><a href="https://yaledatascience.github.io/2016/10/29/autoencoders.html">Guide to Autoencoders</a></li>
<li><a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798">Applied Deep Learning - Part 3: Autoencoders</a></li>
<li><a href="http://deeplearning.net/tutorial/dA.html">Denoising Autoencoders</a></li>
<li><a href="https://skymind.ai/wiki/stacked-denoising-autoencoder">Stacked Denoising Autoencoders</a></li>
<li><a href="https://machinelearningmastery.com/lstm-autoencoders/">A Gentle Introduction to LSTM Autoencoders</a></li>
<li><a href="https://jmetzen.github.io/2015-11-27/vae.html">Variational Autoencoder in TensorFlow</a></li>
<li><a href="https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7">Variational Autoencoders with Tensorflow Probability Layers</a></li>
</ul>
<p><a id="blogs_"></a></p>
<h1 id="blogs">Blogs<a class="headerlink" href="#blogs" title="Permanent link"></a></h1>
<ul>
<li><a href="https://ai.facebook.com/blog/">Facebook AI</a></li>
<li><a href="https://ai.googleblog.com/">Google AI</a></li>
<li><a href="https://deepmind.com/blog">Google DeepMind</a></li>
<li><a href="https://www.deeplearningwizard.com/">Deep Learning Wizard</a></li>
<li><a href="https://towardsdatascience.com/">Towards Data Science</a></li>
<li><a href="https://jalammar.github.io/">Jay Alammar : Visualizing machine learning one concept at a time</a></li>
<li><a href="https://medium.com/inside-machine-learning">Inside Machine Learning: Deep-dive articles about machine learning, cloud, and data. Curated by IBM</a></li>
<li><a href="http://colah.github.io/">colah&rsquo;s blog</a></li>
<li><a href="https://www.jeremyjordan.me/">Jeremy Jordan</a></li>
<li><a href="https://www.svds.com/tag/data-science/">Silicon Valley Data Science</a></li>
<li><a href="https://ikhlestov.github.io/pages/">Illarion’s Notes</a></li>
</ul></article></body></html>